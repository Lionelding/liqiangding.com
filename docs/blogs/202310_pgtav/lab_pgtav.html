<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Works at Irreverent Labs</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link rel="stylesheet" href="../../assets/css/tags.css">
    <style>
        :root {
            --primary-color: #1a1a1a;
            --accent-color: #6366f1;
            --gradient: linear-gradient(135deg, #ff9800 0%, #ffb74d 50%, #ffc107 100%);
            --glass: rgba(255, 255, 255, 0.9);
            --shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Inter', system-ui, -apple-system, sans-serif;
        }

        body {
            background: #f8fafc;
            color: #334155;
            line-height: 1.6;
        }

        nav {
            background: var(--glass);
            backdrop-filter: blur(10px);
            padding: 1rem 5%;
            position: fixed;
            width: 100%;
            top: 0;
            z-index: 1000;
            box-shadow: var(--shadow);
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
            display: flex;
            justify-content: center;
        }

        nav ul {
            display: flex;
            list-style: none;
            gap: 2rem;
            margin: 0;
            padding: 0;
        }

        nav a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            position: relative;
            padding: 0.5rem 0;
        }

        nav a::after {
            content: '';
            position: absolute;
            bottom: 0;
            left: 0;
            width: 0;
            height: 2px;
            background: var(--gradient);
            transition: width 0.3s ease;
        }

        nav a:hover::after {
            width: 100%;
        }

        /* Mobile navigation */
        .menu-toggle {
            display: none;
            flex-direction: column;
            gap: 5px;
            cursor: pointer;
        }

        .menu-toggle span {
            width: 25px;
            height: 3px;
            background: var(--primary-color);
            transition: all 0.3s ease;
        }

        .menu-toggle.active span:nth-child(1) {
            transform: rotate(45deg) translate(5px, 5px);
        }

        .menu-toggle.active span:nth-child(2) {
            opacity: 0;
        }

        .menu-toggle.active span:nth-child(3) {
            transform: rotate(-45deg) translate(5px, -5px);
        }

        @media (max-width: 768px) {
            nav ul {
                display: none;
                flex-direction: column;
                gap: 1rem;
                position: absolute;
                top: 100%;
                left: 0;
                width: 100%;
                background: var(--glass);
                backdrop-filter: blur(10px);
                padding: 1rem;
                box-shadow: var(--shadow);
            }

            nav ul.active {
                display: flex;
            }

            .menu-toggle {
                display: flex;
            }
        }


        .container {
            display: flex;
            margin: 6rem 5% 2rem;
            gap: 3rem;
            max-width: 1200px;
            margin-left: auto;
            margin-right: auto;
        }

        .sidebar {
            width: 300px;
            text-align: center;
            position: sticky;
            top: 6rem;
            height: fit-content;
        }

        /* Updated profile image styles */
        .profile-pic-container {
            position: relative;
            width: 300px;
            height: 300px;
            margin: 0 auto 1.5rem;
        }

        .profile-pic {
            width: 100%;
            height: 100%;
            border-radius: 50%;
            object-fit: cover;
            position: relative;
            z-index: 2;
        }

        .social-icons {
            display: flex;
            justify-content: center;
            gap: 1.5rem;
            margin-top: 1rem;
        }

        .social-icons a {
            width: 40px;
            height: 40px;
            background: var(--glass);
            backdrop-filter: blur(5px);
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            color: var(--primary-color);
            transition: all 0.3s ease;
            box-shadow: var(--shadow);
        }

        .social-icons a:hover {
            background: var(--gradient);
            color: white;
            transform: translateY(-3px);
        }

        /* Updated article styles to match blog listing */
        .blog-content {
            flex: 1;
            max-width: 800px;
        }

        .blog-article {
            background: var(--glass);
            backdrop-filter: blur(10px);
            border-radius: 1.5rem;
            padding: 3rem;
            box-shadow: var(--shadow);
            animation: fadeIn 0.6s ease-out;
        }

        .article-header {
            margin-bottom: 2rem;
        }

        .article-title {
            font-size: 2.5rem;
            color: #1a1a1a;
            line-height: 1.2;
            margin-bottom: 1.5rem;
        }

        .article-subtitle {
            background: #1E90FF;
            /* Change to a blue color */
            -webkit-background-clip: text;
            background-clip: text;
            color: transparent;
            line-height: 1.2;
            margin-top: 1.5rem;
            margin-bottom: 0.8rem;
        }

        .article-meta {
            color: #64748b;
            font-size: 0.95rem;
            display: flex;
            gap: 1.5rem;
            margin-bottom: 2rem;
        }

        .article-image {
            width: 100%;
            height: 100%;
            object-fit: cover;
            border-radius: 1rem;
            margin: 2rem 0;
            border: 3px solid transparent;
            mask-composite: exclude;
        }

        .article-content {
            font-size: 1.1rem;
            line-height: 1.8;
            color: #475569;
        }

        .article-content h2 {
            margin: 2.5rem 0 1.5rem;
            color: var(--primary-color);
            font-size: 1.75rem;
        }

        .article-content code {
            background: rgba(99, 102, 241, 0.1);
            padding: 0.3em 0.5em;
            border-radius: 0.3em;
            font-family: 'Fira Code', monospace;
        }

        .article-content pre {
            background: var(--primary-color);
            color: white;
            padding: 1.5rem;
            border-radius: 0.8rem;
            overflow-x: auto;
            margin: 2rem 0;
        }

        /* Tag styles imported from tags.css */
        .tags {
            margin: 2rem 0;
        }

        .tag {
            padding: 0.5rem 1rem;
            font-size: 0.9rem;
        }

        .back-to-blog {
            display: inline-flex;
            align-items: center;
            gap: 0.75rem;
            color: #3B82F6;
            text-decoration: none;
            font-weight: 500;
            padding: 0.75rem 1.5rem;
            border-radius: 999px;
            background: var(--glass);
            transition: all 0.3s ease;
        }

        .back-to-blog:hover {
            background: var(--gradient);
            color: white;
            transform: translateX(-5px);
        }

        .social-sharing {
            display: flex;
            gap: 1rem;
            margin: 3rem 0 2rem;
        }

        .social-sharing a {
            padding: 0.8rem 1.5rem;
            border-radius: 999px;
            background: var(--glass);
            backdrop-filter: blur(5px);
            display: flex;
            align-items: center;
            gap: 0.75rem;
            color: var(--primary-color);
            text-decoration: none;
            transition: all 0.3s ease;
            box-shadow: var(--shadow);
        }

        .social-sharing a:hover {
            background: var(--gradient);
            color: white;
            transform: translateY(-2px);
        }

        .center-equation {
            text-align: center;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
                transform: translateY(20px);
            }

            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .gif-container {
            display: flex;
            justify-content: space-between;
            gap: 1rem;
            margin: 2rem 0;
        }

        .gif-container img {
            width: 32%;
            border-radius: 1rem;
            box-shadow: var(--shadow);
        }

        .sub-line {
            font-size: 0.9rem;
            color: #64748b;
            /* A lighter color for the sub-line */
            margin-top: 0.5rem;
            /* Add some spacing between the main text and sub-line */
        }

        @media (max-width: 768px) {
            .container {
                flex-direction: column;
                margin-top: 4rem;
            }

            .sidebar {
                width: 100%;
                position: static;
            }

            .profile-pic {
                width: 300px;
                height: 300px;
            }

            .blog-article {
                padding: 2rem;
            }

            .article-title {
                font-size: 2rem;
            }

            .article-image {
                height: 350px;
                margin: 1.5rem 0;
            }

            .social-sharing {
                flex-direction: column;
            }

            .gif-container {
                flex-direction: column;
            }

            .gif-container img {
                width: 100%;
                margin-bottom: 1rem;
            }
        }
    </style>
</head>

<body>
    <!-- Navigation will be injected here by navigation-config.js -->
    <div id="nav-container"></div>

    <div class="container">
        <aside class="sidebar">
            <div class="profile-pic-container">
                <img src="../../assets/images/profile.jpg" alt="Liqiang" class="profile-pic">
            </div>
            <div class="social-icons">
                <a href="https://x.com/liqiang_ding" target="_blank"><i class="fab fa-twitter"></i></a>
                <a href="https://www.youtube.com/@947973" target="_blank"><i class="fab fa-youtube"></i></a>
                <a href="https://www.linkedin.com/in/liqiang-ding-0431" target="_blank"><i
                        class="fab fa-linkedin"></i></a>
                <a href="https://github.com/Lionelding" target="_blank"><i class="fab fa-github"></i></a>
            </div>
        </aside>

        <main class="blog-content">
            <article class="blog-article">
                <header class="article-header">
                    <div class="article-meta">
                        <span>Published: October 01, 2023</span>
                    </div>
                    <h1 class="article-title">Diffusion-Based One-Shot Video Generation via Pose-Guided Temporal
                        Consistency and Spatial Alignment</h1>
                    <img src="./00004_result_r5t5.png" alt="article image" class="article-image">

                </header>

                <div class="tags">
                    <span class="tag">Temporal-consistent</span>
                    <span class="tag">One-shot video generation</span>
                    <span class="tag">Pose-guided</span>

                </div>

                <div
                    style="background-color: #e1f6ea; border-left: 4px solid #ccc; padding: 25px; margin: 10px 0; font-style: italic;">
                    <p>A technical research I architected and executed at Irreverent Labs. For technical
                        details, please see our paper.</p>
                </div>

                <div class="article-content">
                    <h3 class="article-subtitle">Abstract</h3>
                    <p> We propose a novel framework that
                        integrates human pose information as a guiding mechanism for the T2V synthesis process. Our
                        model leverages pose estimation and motion priors to ensure anatomically plausible and
                        contextually relevant human movements, bridging the gap between textual semantics and visual
                        dynamics.
                    </p>
                    <p>
                        We also design two novel pose-specific evaluation metrics, Frame-to-Frame Pose Distance
                        (FFPD) and Optical Flow Difference Score (OFDS), designed to quantitatively evaluate the
                        alignment of generated poses with input text prompts and the realism of motion sequences,
                        respectively. Extensive experiments on benchmark datasets
                        demonstrate that our approach outperforms existing methods in both qualitative and quantitative
                        evaluations, achieving superior control over human pose dynamics while maintaining high video
                        quality. This work not only advances the state-of-the-art in T2V synthesis but also provides a
                        robust framework for applications requiring precise human motion generation, such as virtual
                        avatars, animation, and human-computer interaction.
                    </p>

                    <h3 class="article-subtitle">Previous Challenges</h3>
                    <li> The requirement for a model to generalize across all possible scenes while maintaining
                        high-quality output generation based on input text prompts. This poses a significant challenge
                        due to the substantial divergence between the inference-time data distribution and the
                        training-time distribution, leading to suboptimal generalization performance.</li>
                    <li> The underutilization of critical prior information inherently present in the input data.
                        Despite its availability, the model fails to adequately emphasize or leverage this information,
                        resulting in degraded performance, particularly in tasks requiring temporal consistency. This
                        highlights a gap in the model's ability to effectively integrate and prioritize salient features
                        from the input domain.</li>

                    <h3 class="article-subtitle">Hypothesis</h3>

                    <p>1. A one-shot diffusion model, designed to moderately overfit to a single video-text prompt pair
                        during training, demonstrates the capability to generate contextually faithful video content
                        when provided with a semantically similar text prompt during inference.</p>

                    <p>2. By maintaining consistent pose priors across both the training and inference phases, the model
                        generates video content with significantly improved motion coherence and temporal consistency.
                    </p>

                    <img src="./hypothesis.png" alt="hypothesis" class="article-image">
                    <p style="text-align: center;">Fig: Training input: a video + pose feature + text prompt. Inference
                        input: pose feature + text
                        prompt</p>


                    <h3 class="article-subtitle">Dataset</h3>
                    <div class="gif-container">
                        <img src="./yoga3.gif" alt="GIF 1">
                        <img src="./yoga3_r7t7_pose.gif" alt="GIF 2">
                        <img src="./text.png" alt="GIF 2">

                    </div>

                    <p>Since the model is designed to over-fit to a single video-pose-text triplet, only a single data
                        point
                        is required</p>
                    <li> Train set input: a 24-frame video + a text prompt + human pose</li>
                    <li> Human pose: Inferred with OpenPose on the same input video</li>
                    <li> Diversity: 24 videos in total</li>
                    <li> Input dimension: 128 x 128 x 3 x 24</li>
                    <li> Inference input: a text prompt "Mickey mouse is doing yoga in a room" + human pose</li>

                    <h3 class="article-subtitle">Experiments</h3>
                    <p> Setup</p>
                    <li>1 H100 + 100 epochs of training step (about 5 min)</li>
                    <p> Text prompts at Inference</p>
                    <li> "A lady is doing yoga in a room (<- same as the training input)"</li>
                    <li> "Micky mouse is practising yoga in a room"</li>
                    <li> "Spider man is doing yoga on the beach, cartoon style."</li>
                    <li> "Wonder woman is doing yoga with a hat."</li>
                    <img src="./sample.gif" alt="diagram" class="article-image">
                    <figcaption style="text-align: center;">Figure: Generated results based on the input pose and
                        prompts (Please fresh this page if the animation stops)</figcaption>

                    <h3 class="article-subtitle">Method</h3>
                    <p>1. The workflow and the main builidng blocks</p>
                    <img src="./diagram.png" alt="diagram" class="article-image">
                    <figcaption style="text-align: center;">Figure: Overall structure of the proposed model</figcaption>

                    <img src="./unet.png" alt="unet" class="article-image">
                    <figcaption style="text-align: center;">Figure: UNet structure</figcaption>


                    <img src="./pose_addition.png" alt="pose_addition" class="article-image">
                    <figcaption style="text-align: center;">Figure: The pose features (named adapted features) is
                        merged with the video representation (named hidden states) in the latent space</figcaption>

                    <img src="./full.png" alt="full" class="article-image">
                    <figcaption style="text-align: center;">Figure: A complete block that takes timestamp, latent video
                        variable, and pose features</figcaption>
                    <br></br>

                    <p>2. Classifier-Free Guidance with Pose Information</p>
                    <img src="./cfg_formula.png" alt="cfg_formula" class="article-image">
                    <figcaption style="text-align: center;">Figure: the standard classifier-free guidance (CFG) formula
                    </figcaption>

                    <img src="./cfg_pose.png" alt="cfg_pose" class="article-image">
                    <figcaption style="text-align: center;">Figure: Sampling with pose-guided CFG</figcaption>

                    <img src="./cfg_implementation.png" alt="cfg_implementation" class="article-image">
                    <figcaption style="text-align: center;">Figure: Pose-guided CFG Implementation</figcaption>


                    <p>3. Optical Flow Difference Score</p>
                    <img src="./ofds.png" alt="ofds" class="article-image">
                    <figcaption style="text-align: center;">Figure: A graphic illustration of Optical flow difference
                        score (OFDS)</figcaption>
                    <p>Occasionally, the generated video sequences exhibit artifacts such as the presence of additional
                        limbs or heads in certain frames, indicating inconsistencies in the synthesis process. To
                        quantitatively detect and measure such anomalies, we introduce a novel metric termed the Optical
                        Flow Difference Score (OFDS). This metric computes the optical flow vectors between two
                        consecutive frames \( f_1 \) and \( f_2 \), in the generated video. Using these vectors, we
                        warp \( f_1 \) to
                        produce
                        a predicted frame \( f_{2}' \). The \( L_2 \) norm between \( f_2 \) and \( f_{2}' \) is then
                        calculated to quantify
                        the
                        discrepancy. A low OFDS indicates smooth temporal transitions with minimal pixel-level
                        inconsistencies, while a high OFDS signifies the presence of unexpected
                        artifacts, such as unnatural pixel displacements or structural anomalies. This metric provides a
                        robust mechanism for evaluating the temporal coherence and structural integrity of generated
                        video sequences.</p>


                    <br></br>


                    <p>4. Frame-Frame Pose Distance (FFPD) </p>
                    <p></p>
                    <img src="./ffpd.png" alt="ffpd" class="article-image">
                    <figcaption style="text-align: center;">Figure: A graphic illustration of frame-frame pose distance
                        (FFPD)</figcaption>

                    <p>To quantitatively evaluate the alignment between generated videos and the original training
                        video, we introduce a novel metric termed Frame-Frame Pose Difference (FFPD). Specifically, we
                        employ an off-the-shelf pose estimation model to extract pose keypoints from each frame of both
                        the generated video \( V_{gen} \) and the reference video \( V_{ref} \). Let \( P_{gen}^t \) and
                        \( P_{ref}^t \) denote the sets of pose joint coordinates at frame \( t \) for \( V_{gen} \) and
                        \( V_{ref} \), respectively. The FFPD is computed as the \( L_2 \) norm between corresponding
                        pose keypoints across all frames:</p>

                    <p style="text-align: center;">
                        FFPD =
                        <span style="font-size: 1.2em;">(1/T)</span>
                        <span style="font-size: 1.2em;">&sum;<sub>t=1</sub><sup>T</sup></span>
                        &Vert;P<sub>gen</sub><sup>t</sup> &minus; P<sub>ref</sub><sup>t</sup>&Vert;<sub>2</sub>
                    </p>

                    <p>where
                        \(T \) represents the total number of frames. A high FFPD score indicates significant
                        misalignment
                        between the generated and reference videos, suggesting deviations in pose dynamics. Conversely,
                        a low FFPD score reflects strong alignment, demonstrating that the generated video faithfully
                        preserves the pose structure of the reference video. This metric provides a robust measure for
                        assessing the structural consistency of generated videos in relation to the original training
                        data.</p>

                    <h3 class="article-subtitle">Analysis</h3>

                    <img src="./ofds_result.png" alt="ofds_result" class="article-image">
                    <figcaption style="text-align: center;">Figure: Optical flow difference
                        score (OFDS) computed on the generation with pose guidance and without it </figcaption>
                    <img src="./ffpd_0.png" alt="ffpd_0" class="article-image">
                    <figcaption style="text-align: center;">Figure: The face is more aligned in the pose-guided
                        generated video</figcaption>
                    <img src="./ffpd_2.png" alt="ffpd_2" class="article-image">
                    <figcaption style="text-align: center;">Figure: The arm is more aligned in the pose-guided generated
                        video</figcaption>
                    <img src="./ffpd_quant.png" alt="ffpd_quant" class="article-image">
                    <figcaption style="text-align: center;">Figure: A quantitative comparison between generated results
                        with pose guidance and
                        without it. Pose-guided results are more aligned to the original training video.</figcaption>




                    <h2>References</h2>
                    <ol>
                        <li><a href="https://arxiv.org/abs/2212.11565">Tune-A-Video: One-Shot Tuning of Image Diffusion
                                Models for Text-to-Video Generation</a></li>
                        <li><a href="https://arxiv.org/abs/2302.08453">T2I-Adapter: Learning Adapters to Dig out More
                                Controllable Ability for Text-to-Image Diffusion Models</a></li>
                    </ol>

                    <div style="text-align: center; margin-top: 3rem;">
                        <a href="../blogs.html" class="back-to-blog">
                            <i class="fas fa-arrow-left"></i>
                            Back to Blogs
                        </a>
                    </div>
                </div>

            </article>
        </main>
    </div>

    <!-- External JavaScript -->
    <script src="../../assets/js/navigation.js"></script>
    <script src="../../assets/js/navigation-config.js"></script>
</body>

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<script>
    // Mobile menu toggle
    const mobileMenu = document.getElementById('mobile-menu');
    const navMenu = document.getElementById('nav-menu');

    mobileMenu.addEventListener('click', () => {
        mobileMenu.classList.toggle('active');
        navMenu.classList.toggle('active');
    });
</script>

</html>