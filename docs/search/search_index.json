{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Welcome to the Story of My Kingdom! \u00b6 img { border: 1px solid #ddd; border-radius: 4px; padding: 5px; width: 640px; float: center } My name is Liqiang Ding. I am a cat-lover, a big fun of sci-fi movies and classical music. This blog is a place to share my discoveries from life and work. I wish you enjoy reading it. :)","title":"Home"},{"location":"index.html#welcome-to-the-story-of-my-kingdom","text":"img { border: 1px solid #ddd; border-radius: 4px; padding: 5px; width: 640px; float: center } My name is Liqiang Ding. I am a cat-lover, a big fun of sci-fi movies and classical music. This blog is a place to share my discoveries from life and work. I wish you enjoy reading it. :)","title":"Welcome to the Story of My Kingdom!"},{"location":"about.html","text":"About Myself \u00b6 Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Artificial Intelligence? Computer Vision? Natural language Processing? \u00b6 Artificial Intelligence never fails to appeal to me. Just like Doraemon to Nobita Nobi , I have been dreaming of having a computer friend since I was a kid. It has a bunch of fancy inventions from the 22 nd century in his four dimension pocket, joins me on the adventure exploring this world, and most importantly, teaches by honest words and actions instead of banalities. For years, I scratched numerous drafts about how it would look like, imagined the fancy capabilities it might have, and even came up with scenarios where it could come to realities without scaring my parents. Well, most of them is lost in time, but the fantasy for building such a powerful program still sticks to my mind. Finished the elementary school and high school in China, I was fortunate to came aboard to Canada to continue my study at McGill University, majoring in eletrical engineering. Studying at McGill really broadened my horizons, and it was the real starting point for me to get to know programming algorithms, electrical circuits, information theory and a lot of cool stuff. Being exposure to this massive amount of new knowledge is an exciting yet challenging journey, and no need for mentioning how many nights spent in the Schulich library. However, there was always one question confusing me: What do I want to do with these great pieces of knowledge? Trying to find this answer, I took a few internships across different industries, with various technical focuses. I witnessed how teams across the world leveraged their own experties, contributing to making a great product that faciliated millions of people. I was also amazed to see that people were willing to spend days in improving browsers just to render contents with 1 second faster. On one side, all of these experiences pushed me to go further with my study, and opened completely new worlds. On the other side, they also deepened my question. Time is limited, decisions need to be made. In the last year of my bachelor study, one of my friends mentioned me a course named computer vision that he was about to take. For curiosity, I clicked the course website in the evening, and was greatly surprised of the amazing things that it can do. For the first time, I realized computer algorithms are not only applied to sort an array or to design the shortest path for a busy postman. Instead, it can be so connected to the most of vision-related tasks we perform in the daily life, to recognize a poker card, to restore an old image, and even to stabilize poor-taken videos from amateurs. I took the course without hesitation. It was also around the same time when everyone started to talk deep learning. The success of AlexNet proved that applying the data-hungry iterative process is feasible even in the high dimensional data like images. Millions of dollars were spent by the those tech giants to build AI labs. The power of supervised learning can even be explained clearly by hosts in late night TV shows\u3002 It looked like everything could be solved by deep learning, and the only problem remained as how we can apply this fashionable approach to actually do them. Being fascinated to know all of this, I thought it would be cool to go one step further by taking the graduate school, in order to have a better understanding of how things exactly work. With a mixed feeling for being excited and concerned, I chose the graduate school at McGill over a job offer at a bank. Many years later when I looked back at this decision in those sleepless nights, I really appreciated the choice I made: It is always easier to believe a plausible statement from others, rather than figuring out if the statement is really valid by myself. The Deep learning, which sounds promising and does deliver astonishing performances, also comes with a lot of non-neglectable problems. Through my graduate study, one of my main tasks was to identify these limitations and designed corresponding solutions to mitigate them.","title":"About Myself"},{"location":"about.html#about-myself","text":"Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"About Myself"},{"location":"about.html#artificial-intelligence-computer-vision-natural-language-processing","text":"Artificial Intelligence never fails to appeal to me. Just like Doraemon to Nobita Nobi , I have been dreaming of having a computer friend since I was a kid. It has a bunch of fancy inventions from the 22 nd century in his four dimension pocket, joins me on the adventure exploring this world, and most importantly, teaches by honest words and actions instead of banalities. For years, I scratched numerous drafts about how it would look like, imagined the fancy capabilities it might have, and even came up with scenarios where it could come to realities without scaring my parents. Well, most of them is lost in time, but the fantasy for building such a powerful program still sticks to my mind. Finished the elementary school and high school in China, I was fortunate to came aboard to Canada to continue my study at McGill University, majoring in eletrical engineering. Studying at McGill really broadened my horizons, and it was the real starting point for me to get to know programming algorithms, electrical circuits, information theory and a lot of cool stuff. Being exposure to this massive amount of new knowledge is an exciting yet challenging journey, and no need for mentioning how many nights spent in the Schulich library. However, there was always one question confusing me: What do I want to do with these great pieces of knowledge? Trying to find this answer, I took a few internships across different industries, with various technical focuses. I witnessed how teams across the world leveraged their own experties, contributing to making a great product that faciliated millions of people. I was also amazed to see that people were willing to spend days in improving browsers just to render contents with 1 second faster. On one side, all of these experiences pushed me to go further with my study, and opened completely new worlds. On the other side, they also deepened my question. Time is limited, decisions need to be made. In the last year of my bachelor study, one of my friends mentioned me a course named computer vision that he was about to take. For curiosity, I clicked the course website in the evening, and was greatly surprised of the amazing things that it can do. For the first time, I realized computer algorithms are not only applied to sort an array or to design the shortest path for a busy postman. Instead, it can be so connected to the most of vision-related tasks we perform in the daily life, to recognize a poker card, to restore an old image, and even to stabilize poor-taken videos from amateurs. I took the course without hesitation. It was also around the same time when everyone started to talk deep learning. The success of AlexNet proved that applying the data-hungry iterative process is feasible even in the high dimensional data like images. Millions of dollars were spent by the those tech giants to build AI labs. The power of supervised learning can even be explained clearly by hosts in late night TV shows\u3002 It looked like everything could be solved by deep learning, and the only problem remained as how we can apply this fashionable approach to actually do them. Being fascinated to know all of this, I thought it would be cool to go one step further by taking the graduate school, in order to have a better understanding of how things exactly work. With a mixed feeling for being excited and concerned, I chose the graduate school at McGill over a job offer at a bank. Many years later when I looked back at this decision in those sleepless nights, I really appreciated the choice I made: It is always easier to believe a plausible statement from others, rather than figuring out if the statement is really valid by myself. The Deep learning, which sounds promising and does deliver astonishing performances, also comes with a lot of non-neglectable problems. Through my graduate study, one of my main tasks was to identify these limitations and designed corresponding solutions to mitigate them.","title":"Artificial Intelligence? Computer Vision? Natural language Processing?"},{"location":"backup.html","text":"","title":"Backup"},{"location":"blogs.html","text":"Gradient Class Activation Map (Grad-CAM) Summary \u00b6 The following note demonstrates the logic procedure of how a GradCam is computed. For ... More will be coming... \u00b6","title":"Technical Blogs"},{"location":"blogs.html#gradient-class-activation-map-grad-cam-summary","text":"The following note demonstrates the logic procedure of how a GradCam is computed. For ...","title":"Gradient Class Activation Map (Grad-CAM) Summary"},{"location":"blogs.html#more-will-be-coming","text":"","title":"More will be coming..."},{"location":"gallery.html","text":"Under construction. Coming Soon...","title":"Gallery"},{"location":"listen_and_learn.html","text":"Learning is a life-long process ... \u00b6 img { border: 1px solid #ddd; border-radius: 4px; padding: 5px; width: 640px; float: center }","title":"Listen and Learn"},{"location":"listen_and_learn.html#learning-is-a-life-long-process","text":"img { border: 1px solid #ddd; border-radius: 4px; padding: 5px; width: 640px; float: center }","title":"Learning is a life-long process ..."},{"location":"projects.html","text":"PythonWoW \u00b6 As a machine learning developer/researcher, I encountered many python related questions from time to time. The majority of them are straightforward, but some are subtle to understand, and hard to be applied properly in real practise. During the process for searching answers, I realized that it usually becomes super clear if I write them down with my own examples and understanding. This repository serves as my notes to track of these questions. I usually start from the problem statement raised from a real world example, and goes to the reasons behind it. I would be also very happy if this can help you. Click me for details Tensorflow2 Practises \u00b6 Finally, tensorflow got upgraded to 2.x. It is time to try it out and have some fun :) Click me for details Code practise with Efficient Python \u00b6 This repository contains notes that I took while reading this book. In general, this is a very fruitful journey. The author covers eight major areas about python programming, which are super interesting and especially helpful for my daily work as a machine learning engineer. For these intermediate/senior level technical books, I find that the best way to absorb them is to code as I read them through. Therefore, all the codes in this repo are the examples that the author mentions in his book. Also, since a number of chapters require some additional background knowledge, I also write them down together with my own interpretation in separate files, which live in the relevant folders. Anyway, I will be very glad if you find these examples are helpful for your projects, or you just have some fun reading them. Please feel free to send me an email if you find a mistake or you don't agree with my interpretations. Click me for details Frequency \u00b6 A video receiver with analog circuits. Wait till the end for an easter egg (if you are big a fun of the movie Inception ) When Beethoven Meets VHDL \u00b6 A music box with digital circuits. McGill Gear Solid \u00b6 Led a team of 6 from different engineering faculties to design a robot in 7 weeks. The robot is able to localize, navigate, avoid obstacles and rescue blocks on 365*365 cm maps by using a single processor and sensors","title":"Personal Projects"},{"location":"projects.html#pythonwow","text":"As a machine learning developer/researcher, I encountered many python related questions from time to time. The majority of them are straightforward, but some are subtle to understand, and hard to be applied properly in real practise. During the process for searching answers, I realized that it usually becomes super clear if I write them down with my own examples and understanding. This repository serves as my notes to track of these questions. I usually start from the problem statement raised from a real world example, and goes to the reasons behind it. I would be also very happy if this can help you. Click me for details","title":"PythonWoW"},{"location":"projects.html#tensorflow2-practises","text":"Finally, tensorflow got upgraded to 2.x. It is time to try it out and have some fun :) Click me for details","title":"Tensorflow2 Practises"},{"location":"projects.html#code-practise-with-efficient-python","text":"This repository contains notes that I took while reading this book. In general, this is a very fruitful journey. The author covers eight major areas about python programming, which are super interesting and especially helpful for my daily work as a machine learning engineer. For these intermediate/senior level technical books, I find that the best way to absorb them is to code as I read them through. Therefore, all the codes in this repo are the examples that the author mentions in his book. Also, since a number of chapters require some additional background knowledge, I also write them down together with my own interpretation in separate files, which live in the relevant folders. Anyway, I will be very glad if you find these examples are helpful for your projects, or you just have some fun reading them. Please feel free to send me an email if you find a mistake or you don't agree with my interpretations. Click me for details","title":"Code practise with Efficient Python"},{"location":"projects.html#frequency","text":"A video receiver with analog circuits. Wait till the end for an easter egg (if you are big a fun of the movie Inception )","title":"Frequency"},{"location":"projects.html#when-beethoven-meets-vhdl","text":"A music box with digital circuits.","title":"When Beethoven Meets VHDL"},{"location":"projects.html#mcgill-gear-solid","text":"Led a team of 6 from different engineering faculties to design a robot in 7 weeks. The robot is able to localize, navigate, avoid obstacles and rescue blocks on 365*365 cm maps by using a single processor and sensors","title":"McGill Gear Solid"},{"location":"pub.html","text":"My Research Interests \u00b6 Google scholar Task Adaptive Metric Space for Medium-Shot Medical Image Classification \u00b6 22th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2019 Full paper poster Artificial Intelligence for Real-Time Multiple Polyp Detection with Identification, Tracking, and Optical Biopsy During Colonoscopy \u00b6 Journal of Gastroenterology, 2019 Full paper A Single Framework for Domain Adaptation and Generalization in Medical Image Analysis \u00b6 Montreal AI Symposium (MAIS), 2018 Full paper poster Apparatus and method for detecting, classifying and tracking road users on frames of video data \u00b6 US Patent, Application US16/535,547 events Full description Deep-learning-based multiple object tracking in traffic surveillance video \u00b6 Master Thesis, McGill University, 2018 Full paper Generation of spatial-temporal panoramas with a single moving camera \u00b6 13 th Conference on Computer and Robot Vision (CRV), 2016 Full paper Video","title":"Research & Publication"},{"location":"pub.html#my-research-interests","text":"Google scholar","title":"My Research Interests"},{"location":"pub.html#task-adaptive-metric-space-for-medium-shot-medical-image-classification","text":"22th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2019 Full paper poster","title":"Task Adaptive Metric Space for Medium-Shot Medical Image Classification"},{"location":"pub.html#artificial-intelligence-for-real-time-multiple-polyp-detection-with-identification-tracking-and-optical-biopsy-during-colonoscopy","text":"Journal of Gastroenterology, 2019 Full paper","title":"Artificial Intelligence for Real-Time Multiple Polyp Detection with Identification, Tracking, and Optical Biopsy During Colonoscopy"},{"location":"pub.html#a-single-framework-for-domain-adaptation-and-generalization-in-medical-image-analysis","text":"Montreal AI Symposium (MAIS), 2018 Full paper poster","title":"A Single Framework for Domain Adaptation and Generalization in Medical Image Analysis"},{"location":"pub.html#apparatus-and-method-for-detecting-classifying-and-tracking-road-users-on-frames-of-video-data","text":"US Patent, Application US16/535,547 events Full description","title":"Apparatus and method for detecting, classifying and tracking road users on frames of video data"},{"location":"pub.html#deep-learning-based-multiple-object-tracking-in-traffic-surveillance-video","text":"Master Thesis, McGill University, 2018 Full paper","title":"Deep-learning-based multiple object tracking in traffic surveillance video"},{"location":"pub.html#generation-of-spatial-temporal-panoramas-with-a-single-moving-camera","text":"13 th Conference on Computer and Robot Vision (CRV), 2016 Full paper Video","title":"Generation of spatial-temporal panoramas with a single moving camera"},{"location":"read.html","text":"","title":"Read"},{"location":"MyBlogs/GradCam.html","text":"Gradient Class Activation Map (Grad-CAM) Summary \u00b6 The following note demonstrates the logic procedure of how a GradCam is computed. For details, please see: Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization Assumption \u00b6 Class Activation Mapping \u00b6 y^{c} = \\sum\\limits_{k} w^k_c \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} A^k_{ij} The penultimate layer produces K K activation map A^K A^K with width u u and height v v . The feature goes through \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} , which is called global averaged pooled , and then linearly transforemd to y^{c} y^{c} for each class c c . Procedure \u00b6 1. Choose the penultimate layer. \u00b6 This is the layer where we want to visualize the activation map. The reason why it is called the penultimate layer is because the last layer of a model is usually the layer computing class-specific loss (i.e. Sigmoid loss, softmax loss, etc). The penultimate layer is a layer, which is before the last layer, that we want to compute the loss respect to. 2. Compute the class-specific loss (i.e. loss of y^{c} y^{c} ) on the last layer of the model. \u00b6 3. Compute the gradients of y^c y^c respect to the penultimate layer's activation map A^K A^K . \u00b6 \\alpha^c_k = \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} \\frac{\\partial y^c}{\\partial A^k_{ij}} The dimension of the gradients is then reduced from (batch_size, u, v, channel) to (channel) by averaging the values in the first three axises. In the paper, this dimension reduction is called global-average-pooling . This result denoted as \\alpha^c_k \\alpha^c_k represents a partial linearization of the penultimate layer to the loss, which captures the importance of the activation A^k A^k for a target c c . 4. Elemently-wise multpile the the gradient with the Activation map. \u00b6 5. Remove any negative results by applying the Relu function over the result. \u00b6 Only interested in the positive influence on the class of interest. (i.e. Pixels whose intensity should increase in order to increase y^{c} y^{c} ) L^c_{Grad-CAM} = Relu(\\sum\\limits_{k} \\alpha^c_k A^k)","title":"Gradient Class Activation Map (Grad-CAM) Summary"},{"location":"MyBlogs/GradCam.html#gradient-class-activation-map-grad-cam-summary","text":"The following note demonstrates the logic procedure of how a GradCam is computed. For details, please see: Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization","title":"Gradient Class Activation Map (Grad-CAM) Summary"},{"location":"MyBlogs/GradCam.html#assumption","text":"","title":"Assumption"},{"location":"MyBlogs/GradCam.html#class-activation-mapping","text":"y^{c} = \\sum\\limits_{k} w^k_c \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} A^k_{ij} The penultimate layer produces K K activation map A^K A^K with width u u and height v v . The feature goes through \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} , which is called global averaged pooled , and then linearly transforemd to y^{c} y^{c} for each class c c .","title":"Class Activation Mapping"},{"location":"MyBlogs/GradCam.html#procedure","text":"","title":"Procedure"},{"location":"MyBlogs/GradCam.html#1-choose-the-penultimate-layer","text":"This is the layer where we want to visualize the activation map. The reason why it is called the penultimate layer is because the last layer of a model is usually the layer computing class-specific loss (i.e. Sigmoid loss, softmax loss, etc). The penultimate layer is a layer, which is before the last layer, that we want to compute the loss respect to.","title":"1. Choose the penultimate layer."},{"location":"MyBlogs/GradCam.html#2-compute-the-class-specific-loss-ie-loss-of-ycyc-on-the-last-layer-of-the-model","text":"","title":"2. Compute the class-specific loss (i.e. loss of y^{c}y^{c}) on the last layer of the model."},{"location":"MyBlogs/GradCam.html#3-compute-the-gradients-of-ycyc-respect-to-the-penultimate-layers-activation-map-akak","text":"\\alpha^c_k = \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} \\frac{\\partial y^c}{\\partial A^k_{ij}} The dimension of the gradients is then reduced from (batch_size, u, v, channel) to (channel) by averaging the values in the first three axises. In the paper, this dimension reduction is called global-average-pooling . This result denoted as \\alpha^c_k \\alpha^c_k represents a partial linearization of the penultimate layer to the loss, which captures the importance of the activation A^k A^k for a target c c .","title":"3. Compute the gradients of y^cy^c respect to the penultimate layer's activation map A^KA^K."},{"location":"MyBlogs/GradCam.html#4-elemently-wise-multpile-the-the-gradient-with-the-activation-map","text":"","title":"4. Elemently-wise multpile the the gradient with the Activation map."},{"location":"MyBlogs/GradCam.html#5-remove-any-negative-results-by-applying-the-relu-function-over-the-result","text":"Only interested in the positive influence on the class of interest. (i.e. Pixels whose intensity should increase in order to increase y^{c} y^{c} ) L^c_{Grad-CAM} = Relu(\\sum\\limits_{k} \\alpha^c_k A^k)","title":"5. Remove any negative results by applying the Relu function over the result."}]}