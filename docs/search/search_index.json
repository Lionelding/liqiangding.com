{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to My Home Page \u00b6 Welcome to my personal website! My name is Liqiang Ding. ![Goolge Scholar] ![Youtube] Github","title":"Home"},{"location":"#welcome-to-my-home-page","text":"Welcome to my personal website! My name is Liqiang Ding. ![Goolge Scholar] ![Youtube] Github","title":"Welcome to My Home Page"},{"location":"about/","text":"Fores tacitus \u00b6 Contra equis vivacis timor \u00b6 Modo non voce subsidit finire multis auro \u00b6 Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"About"},{"location":"about/#fores-tacitus","text":"","title":"Fores tacitus"},{"location":"about/#contra-equis-vivacis-timor","text":"","title":"Contra equis vivacis timor"},{"location":"about/#modo-non-voce-subsidit-finire-multis-auro","text":"Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Modo non voce subsidit finire multis auro"},{"location":"blogs/","text":"Hello World \u00b6 Gradient Class Activation Map (Grad-CAM) Summary \u00b6 Details Blog Title 2 \u00b6 Blog Title 3 \u00b6 Blog Title 4 \u00b6","title":"Blogs"},{"location":"blogs/#hello-world","text":"","title":"Hello World"},{"location":"blogs/#gradient-class-activation-map-grad-cam-summary","text":"Details","title":"Gradient Class Activation Map (Grad-CAM) Summary"},{"location":"blogs/#blog-title-2","text":"","title":"Blog Title 2"},{"location":"blogs/#blog-title-3","text":"","title":"Blog Title 3"},{"location":"blogs/#blog-title-4","text":"","title":"Blog Title 4"},{"location":"projects/","text":"Coming...","title":"Projects"},{"location":"pub/","text":"My Research Interests \u00b6 Papers \u00b6 Artificial Intelligence for Real-Time Multiple Polyp Detection with Identification, Tracking, and Optical Biopsy During Colonoscopy Task Adaptive Metric Space for Medium-Shot Medical Image Classification Deep-learning-based multiple object tracking in traffic surveillance video Generation of spatial-temporal panoramas with a single moving camera Patents \u00b6 Apparatus and method for detecting, classifying and tracking road users on frames of video data","title":"Publications"},{"location":"pub/#my-research-interests","text":"","title":"My Research Interests"},{"location":"pub/#papers","text":"Artificial Intelligence for Real-Time Multiple Polyp Detection with Identification, Tracking, and Optical Biopsy During Colonoscopy Task Adaptive Metric Space for Medium-Shot Medical Image Classification Deep-learning-based multiple object tracking in traffic surveillance video Generation of spatial-temporal panoramas with a single moving camera","title":"Papers"},{"location":"pub/#patents","text":"Apparatus and method for detecting, classifying and tracking road users on frames of video data","title":"Patents"},{"location":"MyBlogs/GradCam/","text":"Gradient Class Activation Map (Grad-CAM) Summary \u00b6 The following note demonstrates the logic procedure of how a GradCam is computed. For details, please see: Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization Assumption \u00b6 Class Activation Mapping \u00b6 y^{c} = \\sum\\limits_{k} w^k_c \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} A^k_{ij} The penultimate layer produces K K activation map A^K A^K with width u u and height v v . The feature goes through \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} , which is called global averaged pooled , and then linearly transforemd to y^{c} y^{c} for each class c c . Procedure \u00b6 1. Choose the penultimate layer. \u00b6 This is the layer where we want to visualize the activation map. The reason why it is called the penultimate layer is because the last layer of a model is usually the layer computing class-specific loss (i.e. Sigmoid loss, softmax loss, etc). The penultimate layer is a layer, which is before the last layer, that we want to compute the loss respect to. 2. Compute the class-specific loss (i.e. loss of y^{c} y^{c} ) on the last layer of the model. \u00b6 3. Compute the gradients of y^c y^c respect to the penultimate layer's activation map A^K A^K . \u00b6 \\alpha^c_k = \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} \\frac{\\partial y^c}{\\partial A^k_{ij}} The dimension of the gradients is then reduced from (batch_size, u, v, channel) to (channel) by averaging the values in the first three axises. In the paper, this dimension reduction is called global-average-pooling . This result denoted as \\alpha^c_k \\alpha^c_k represents a partial linearization of the penultimate layer to the loss, which captures the importance of the activation A^k A^k for a target c c . 4. Elemently-wise multpile the the gradient with the Activation map. \u00b6 5. Remove any negative results by applying the Relu function over the result. \u00b6 Only interested in the positive influence on the class of interest. (i.e. Pixels whose intensity should increase in order to increase y^{c} y^{c} ) L^c_{Grad-CAM} = Relu(\\sum\\limits_{k} \\alpha^c_k A^k)","title":"Gradient Class Activation Map (Grad-CAM) Summary"},{"location":"MyBlogs/GradCam/#gradient-class-activation-map-grad-cam-summary","text":"The following note demonstrates the logic procedure of how a GradCam is computed. For details, please see: Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization","title":"Gradient Class Activation Map (Grad-CAM) Summary"},{"location":"MyBlogs/GradCam/#assumption","text":"","title":"Assumption"},{"location":"MyBlogs/GradCam/#class-activation-mapping","text":"y^{c} = \\sum\\limits_{k} w^k_c \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} A^k_{ij} The penultimate layer produces K K activation map A^K A^K with width u u and height v v . The feature goes through \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} , which is called global averaged pooled , and then linearly transforemd to y^{c} y^{c} for each class c c .","title":"Class Activation Mapping"},{"location":"MyBlogs/GradCam/#procedure","text":"","title":"Procedure"},{"location":"MyBlogs/GradCam/#1-choose-the-penultimate-layer","text":"This is the layer where we want to visualize the activation map. The reason why it is called the penultimate layer is because the last layer of a model is usually the layer computing class-specific loss (i.e. Sigmoid loss, softmax loss, etc). The penultimate layer is a layer, which is before the last layer, that we want to compute the loss respect to.","title":"1. Choose the penultimate layer."},{"location":"MyBlogs/GradCam/#2-compute-the-class-specific-loss-ie-loss-of-ycyc-on-the-last-layer-of-the-model","text":"","title":"2. Compute the class-specific loss (i.e. loss of y^{c}y^{c}) on the last layer of the model."},{"location":"MyBlogs/GradCam/#3-compute-the-gradients-of-ycyc-respect-to-the-penultimate-layers-activation-map-akak","text":"\\alpha^c_k = \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} \\frac{\\partial y^c}{\\partial A^k_{ij}} The dimension of the gradients is then reduced from (batch_size, u, v, channel) to (channel) by averaging the values in the first three axises. In the paper, this dimension reduction is called global-average-pooling . This result denoted as \\alpha^c_k \\alpha^c_k represents a partial linearization of the penultimate layer to the loss, which captures the importance of the activation A^k A^k for a target c c .","title":"3. Compute the gradients of y^cy^c respect to the penultimate layer's activation map A^KA^K."},{"location":"MyBlogs/GradCam/#4-elemently-wise-multpile-the-the-gradient-with-the-activation-map","text":"","title":"4. Elemently-wise multpile the the gradient with the Activation map."},{"location":"MyBlogs/GradCam/#5-remove-any-negative-results-by-applying-the-relu-function-over-the-result","text":"Only interested in the positive influence on the class of interest. (i.e. Pixels whose intensity should increase in order to increase y^{c} y^{c} ) L^c_{Grad-CAM} = Relu(\\sum\\limits_{k} \\alpha^c_k A^k)","title":"5. Remove any negative results by applying the Relu function over the result."}]}