{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Story of My Kingdom! \u00b6 img { border: 1px solid #ddd; border-radius: 4px; padding: 5px; width: 640px; float: center } My name is Liqiang Ding. I am a cat-lover, a big fun of sci-fi movies and classical music. This blog is a place to share my discoveries from life and work. I wish you enjoy reading it. :)","title":"Home"},{"location":"#welcome-to-the-story-of-my-kingdom","text":"img { border: 1px solid #ddd; border-radius: 4px; padding: 5px; width: 640px; float: center } My name is Liqiang Ding. I am a cat-lover, a big fun of sci-fi movies and classical music. This blog is a place to share my discoveries from life and work. I wish you enjoy reading it. :)","title":"Welcome to the Story of My Kingdom!"},{"location":"about/","text":"About Myself \u00b6 Artificial Intelligence? Computer Vision? Natural language Processing? \u00b6 Artificial Intelligence never fails to appeal to me. Just like Doraemon to Nobita Nobi , I have been dreaming of having a computer friend since I was a kid. It has a bunch of fancy inventions from the 22 nd century in his four dimension pocket, joins me on the adventure exploring this world, and most importantly, teaches by honest words and actions instead of banalities. For years, I scratched numerous drafts about how it would look like, imagined the fancy capabilities it might have, and even came up with scenarios where it could come to realities without scaring my parents. Well, most of them is lost in time, but the fantasy for building such a powerful program still sticks to my mind. Finished the elementary school and high school in China, I was fortunate to came aboard to Canada to continue my study at McGill University, majoring in eletrical engineering. Studying at McGill really broadened my horizons, and it was the real starting point for me to get to know programming algorithms, electrical circuits, information theory and a lot of cool stuff. Being exposure to this massive amount of new knowledge is an exciting yet challenging journey, and no need for mentioning how many nights spent in the Schulich library. However, there was always one question confusing me: What do I want to do with these great pieces of knowledge? Trying to find this answer, I took a few internships across different industries, with various technical focuses. I witnessed how teams across the world leveraged their own experties, contributing to making a great product that faciliated millions of people. I was also amazed to see that people were willing to spend days in improving browsers just to render contents with 1 second faster. On one side, all of these experiences pushed me to go further with my study, and opened completely new worlds. On the other side, they also deepened my question. Time is limited, decisions need to be made. In the last year of my bachelor study, one of my friends mentioned me a course named computer vision that he was about to take. For curiosity, I clicked the course website in the evening, and was greatly surprised of the amazing things that it can do. For the first time, I realized computer algorithms are not only applied to sort an array or to design the shortest path for a busy postman. Instead, it can be so connected to the most of vision-related tasks we perform in the daily life, to recognize a poker card, to restore an old image, and even to stabilize poor-taken videos from amateurs. I took the course without hesitation. It was also around the same time when everyone started to talk deep learning. The success of AlexNet proved that applying the data-hungry iterative process is feasible even in the high dimensional data like images. Millions of dollars were spent by the those tech giants to build AI labs. The power of supervised learning can even be explained clearly by hosts in late night TV shows\u3002 It looked like everything could be solved by deep learning, and the only problem remained as how we can apply this fashionable approach to actually do them. Being fascinated to know all of this, I thought it would be cool to go one step further by taking the graduate school, in order to have a better understanding of how things exactly work. With a mixed feeling for being excited and concerned, I chose the graduate school at McGill over a job offer at a bank. Many years later when I looked back at this decision in those sleepless nights, I really appreciated the choice I made: It is always easier to believe a plausible statement from others, rather than figuring out if the statement is really valid by myself. The Deep learning, which sounds promising and does deliver astonishing performances, also comes with a lot of non-neglectable problems. Through my graduate study, one of my main tasks was to identify these limitations and designed corresponding solutions to mitigate them.","title":"About Myself"},{"location":"about/#about-myself","text":"","title":"About Myself"},{"location":"about/#artificial-intelligence-computer-vision-natural-language-processing","text":"Artificial Intelligence never fails to appeal to me. Just like Doraemon to Nobita Nobi , I have been dreaming of having a computer friend since I was a kid. It has a bunch of fancy inventions from the 22 nd century in his four dimension pocket, joins me on the adventure exploring this world, and most importantly, teaches by honest words and actions instead of banalities. For years, I scratched numerous drafts about how it would look like, imagined the fancy capabilities it might have, and even came up with scenarios where it could come to realities without scaring my parents. Well, most of them is lost in time, but the fantasy for building such a powerful program still sticks to my mind. Finished the elementary school and high school in China, I was fortunate to came aboard to Canada to continue my study at McGill University, majoring in eletrical engineering. Studying at McGill really broadened my horizons, and it was the real starting point for me to get to know programming algorithms, electrical circuits, information theory and a lot of cool stuff. Being exposure to this massive amount of new knowledge is an exciting yet challenging journey, and no need for mentioning how many nights spent in the Schulich library. However, there was always one question confusing me: What do I want to do with these great pieces of knowledge? Trying to find this answer, I took a few internships across different industries, with various technical focuses. I witnessed how teams across the world leveraged their own experties, contributing to making a great product that faciliated millions of people. I was also amazed to see that people were willing to spend days in improving browsers just to render contents with 1 second faster. On one side, all of these experiences pushed me to go further with my study, and opened completely new worlds. On the other side, they also deepened my question. Time is limited, decisions need to be made. In the last year of my bachelor study, one of my friends mentioned me a course named computer vision that he was about to take. For curiosity, I clicked the course website in the evening, and was greatly surprised of the amazing things that it can do. For the first time, I realized computer algorithms are not only applied to sort an array or to design the shortest path for a busy postman. Instead, it can be so connected to the most of vision-related tasks we perform in the daily life, to recognize a poker card, to restore an old image, and even to stabilize poor-taken videos from amateurs. I took the course without hesitation. It was also around the same time when everyone started to talk deep learning. The success of AlexNet proved that applying the data-hungry iterative process is feasible even in the high dimensional data like images. Millions of dollars were spent by the those tech giants to build AI labs. The power of supervised learning can even be explained clearly by hosts in late night TV shows\u3002 It looked like everything could be solved by deep learning, and the only problem remained as how we can apply this fashionable approach to actually do them. Being fascinated to know all of this, I thought it would be cool to go one step further by taking the graduate school, in order to have a better understanding of how things exactly work. With a mixed feeling for being excited and concerned, I chose the graduate school at McGill over a job offer at a bank. Many years later when I looked back at this decision in those sleepless nights, I really appreciated the choice I made: It is always easier to believe a plausible statement from others, rather than figuring out if the statement is really valid by myself. The Deep learning, which sounds promising and does deliver astonishing performances, also comes with a lot of non-neglectable problems. Through my graduate study, one of my main tasks was to identify these limitations and designed corresponding solutions to mitigate them.","title":"Artificial Intelligence? Computer Vision? Natural language Processing?"},{"location":"backup/","text":"","title":"Backup"},{"location":"blogs/","text":"Paper Reading: Human Pose Regression with Residual Log-likelihood Estimation \u00b6 Keywords Human pose estimation, laplace, gaussian priors Full Articale Normalizing Flows and Its Friends (Part 2) \u00b6 Keywords flow-based model, RealNVP Full Articale Normalizing Flows and Its Friends (Part 1) \u00b6 Keywords Jacobian Matrix, Determinant, Change of Variable Theorem Full Articale Gradient Class Activation Map (Grad-CAM) Summary \u00b6 Keywords GradCam, penultimate layer Full Articale","title":"Technical Blogs"},{"location":"blogs/#paper-reading-human-pose-regression-with-residual-log-likelihood-estimation","text":"Keywords Human pose estimation, laplace, gaussian priors Full Articale","title":"Paper Reading: Human Pose Regression with Residual Log-likelihood Estimation"},{"location":"blogs/#normalizing-flows-and-its-friends-part-2","text":"Keywords flow-based model, RealNVP Full Articale","title":"Normalizing Flows and Its Friends (Part 2)"},{"location":"blogs/#normalizing-flows-and-its-friends-part-1","text":"Keywords Jacobian Matrix, Determinant, Change of Variable Theorem Full Articale","title":"Normalizing Flows and Its Friends (Part 1)"},{"location":"blogs/#gradient-class-activation-map-grad-cam-summary","text":"Keywords GradCam, penultimate layer Full Articale","title":"Gradient Class Activation Map (Grad-CAM) Summary"},{"location":"gallery/","text":"Under construction. Coming Soon...","title":"Gallery"},{"location":"listen_and_learn/","text":"Learning is a life-long process ... \u00b6 ML Operations Generative Adversariel Networks Certificate Generative Adversariel Networks Course 1 Generative Adversariel Networks Course 2 Generative Adversariel Networks Course 3 Sequence Models Convolutional Neural Networks Probabilistic Graphics Model","title":"Listen and Learn"},{"location":"listen_and_learn/#learning-is-a-life-long-process","text":"ML Operations Generative Adversariel Networks Certificate Generative Adversariel Networks Course 1 Generative Adversariel Networks Course 2 Generative Adversariel Networks Course 3 Sequence Models Convolutional Neural Networks Probabilistic Graphics Model","title":"Learning is a life-long process ..."},{"location":"projects/","text":"PythonWoW \u00b6 As a machine learning developer/researcher, I encountered many python related questions from time to time. The majority of them are straightforward, but some are subtle to understand, and hard to be applied properly in real practise. During the process for searching answers, I realized that it usually becomes super clear if I write them down with my own examples and understanding. This repository serves as my notes to track of these questions. I usually start from the problem statement raised from a real world example, and goes to the reasons behind it. I would be also very happy if this can help you. Click me for details Tensorflow2 Practises \u00b6 Finally, tensorflow got upgraded to 2.x. It is time to try it out and have some fun :) Click me for details Code practise with Efficient Python \u00b6 This repository contains notes that I took while reading this book. In general, this is a very fruitful journey. The author covers eight major areas about python programming, which are super interesting and especially helpful for my daily work as a machine learning engineer. For these intermediate/senior level technical books, I find that the best way to absorb them is to code as I read them through. Therefore, all the codes in this repo are the examples that the author mentions in his book. Also, since a number of chapters require some additional background knowledge, I also write them down together with my own interpretation in separate files, which live in the relevant folders. Anyway, I will be very glad if you find these examples are helpful for your projects, or you just have some fun reading them. Please feel free to send me an email if you find a mistake or you don't agree with my interpretations. Click me for details Frequency \u00b6 A video receiver with analog circuits. Wait till the end for an easter egg (if you are big a fun of the movie Inception ) When Beethoven Meets VHDL \u00b6 A music box with digital circuits. McGill Gear Solid \u00b6 Led a team of 6 from different engineering faculties to design a robot in 7 weeks. The robot is able to localize, navigate, avoid obstacles and rescue blocks on 365*365 cm maps by using a single processor and sensors","title":"Personal Projects"},{"location":"projects/#pythonwow","text":"As a machine learning developer/researcher, I encountered many python related questions from time to time. The majority of them are straightforward, but some are subtle to understand, and hard to be applied properly in real practise. During the process for searching answers, I realized that it usually becomes super clear if I write them down with my own examples and understanding. This repository serves as my notes to track of these questions. I usually start from the problem statement raised from a real world example, and goes to the reasons behind it. I would be also very happy if this can help you. Click me for details","title":"PythonWoW"},{"location":"projects/#tensorflow2-practises","text":"Finally, tensorflow got upgraded to 2.x. It is time to try it out and have some fun :) Click me for details","title":"Tensorflow2 Practises"},{"location":"projects/#code-practise-with-efficient-python","text":"This repository contains notes that I took while reading this book. In general, this is a very fruitful journey. The author covers eight major areas about python programming, which are super interesting and especially helpful for my daily work as a machine learning engineer. For these intermediate/senior level technical books, I find that the best way to absorb them is to code as I read them through. Therefore, all the codes in this repo are the examples that the author mentions in his book. Also, since a number of chapters require some additional background knowledge, I also write them down together with my own interpretation in separate files, which live in the relevant folders. Anyway, I will be very glad if you find these examples are helpful for your projects, or you just have some fun reading them. Please feel free to send me an email if you find a mistake or you don't agree with my interpretations. Click me for details","title":"Code practise with Efficient Python"},{"location":"projects/#frequency","text":"A video receiver with analog circuits. Wait till the end for an easter egg (if you are big a fun of the movie Inception )","title":"Frequency"},{"location":"projects/#when-beethoven-meets-vhdl","text":"A music box with digital circuits.","title":"When Beethoven Meets VHDL"},{"location":"projects/#mcgill-gear-solid","text":"Led a team of 6 from different engineering faculties to design a robot in 7 weeks. The robot is able to localize, navigate, avoid obstacles and rescue blocks on 365*365 cm maps by using a single processor and sensors","title":"McGill Gear Solid"},{"location":"pub/","text":"My Research Interests \u00b6 My research focuses on representation learning with 3D data, self-supervised learning, few-shot learning, and domain adaption. Google scholar Task Adaptive Metric Space for Medium-Shot Medical Image Classification \u00b6 22th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2019 Full paper poster Artificial Intelligence for Real-Time Multiple Polyp Detection with Identification, Tracking, and Optical Biopsy During Colonoscopy \u00b6 Journal of Gastroenterology, 2019 Full paper A Single Framework for Domain Adaptation and Generalization in Medical Image Analysis \u00b6 Montreal AI Symposium (MAIS), 2018 Full paper poster Apparatus and method for detecting, classifying and tracking road users on frames of video data \u00b6 US Patent, Application US16/535,547 events Full description Deep-learning-based multiple object tracking in traffic surveillance video \u00b6 Master Thesis, McGill University, 2018 Full paper Generation of spatial-temporal panoramas with a single moving camera \u00b6 13 th Conference on Computer and Robot Vision (CRV), 2016 Full paper Video","title":"Research & Publication"},{"location":"pub/#my-research-interests","text":"My research focuses on representation learning with 3D data, self-supervised learning, few-shot learning, and domain adaption. Google scholar","title":"My Research Interests"},{"location":"pub/#task-adaptive-metric-space-for-medium-shot-medical-image-classification","text":"22th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2019 Full paper poster","title":"Task Adaptive Metric Space for Medium-Shot Medical Image Classification"},{"location":"pub/#artificial-intelligence-for-real-time-multiple-polyp-detection-with-identification-tracking-and-optical-biopsy-during-colonoscopy","text":"Journal of Gastroenterology, 2019 Full paper","title":"Artificial Intelligence for Real-Time Multiple Polyp Detection with Identification, Tracking, and Optical Biopsy During Colonoscopy"},{"location":"pub/#a-single-framework-for-domain-adaptation-and-generalization-in-medical-image-analysis","text":"Montreal AI Symposium (MAIS), 2018 Full paper poster","title":"A Single Framework for Domain Adaptation and Generalization in Medical Image Analysis"},{"location":"pub/#apparatus-and-method-for-detecting-classifying-and-tracking-road-users-on-frames-of-video-data","text":"US Patent, Application US16/535,547 events Full description","title":"Apparatus and method for detecting, classifying and tracking road users on frames of video data"},{"location":"pub/#deep-learning-based-multiple-object-tracking-in-traffic-surveillance-video","text":"Master Thesis, McGill University, 2018 Full paper","title":"Deep-learning-based multiple object tracking in traffic surveillance video"},{"location":"pub/#generation-of-spatial-temporal-panoramas-with-a-single-moving-camera","text":"13 th Conference on Computer and Robot Vision (CRV), 2016 Full paper Video","title":"Generation of spatial-temporal panoramas with a single moving camera"},{"location":"read/","text":"","title":"Read"},{"location":"MyBlogs/GradCam/","text":"Gradient Class Activation Map (Grad-CAM) Summary \u00b6 The following note demonstrates the logic procedure of how a GradCam is computed. For details, please see: Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization Assumption \u00b6 Class Activation Mapping \u00b6 y^{c} = \\sum\\limits_{k} w^k_c \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} A^k_{ij} The penultimate layer produces K K activation map A^K A^K with width u u and height v v . The feature goes through \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} , which is called global averaged pooled , and then linearly transforemd to y^{c} y^{c} for each class c c . Procedure \u00b6 1. Choose the penultimate layer. \u00b6 This is the layer where we want to visualize the activation map. The reason why it is called the penultimate layer is because the last layer of a model is usually the layer computing class-specific loss (i.e. Sigmoid loss, softmax loss, etc). The penultimate layer is a layer, which is before the last layer, that we want to compute the loss respect to. 2. Compute the class-specific loss (i.e. loss of y^{c} y^{c} ) on the last layer of the model. \u00b6 3. Compute the gradients of y^c y^c respect to the penultimate layer's activation map A^K A^K . \u00b6 \\alpha^c_k = \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} \\frac{\\partial y^c}{\\partial A^k_{ij}} The dimension of the gradients is then reduced from (batch_size, u, v, channel) to (channel) by averaging the values in the first three axises. In the paper, this dimension reduction is called global-average-pooling . This result denoted as \\alpha^c_k \\alpha^c_k represents a partial linearization of the penultimate layer to the loss, which captures the importance of the activation A^k A^k for a target c c . 4. Elemently-wise multpile the the gradient with the Activation map. \u00b6 5. Remove any negative results by applying the Relu function over the result. \u00b6 Only interested in the positive influence on the class of interest. (i.e. Pixels whose intensity should increase in order to increase y^{c} y^{c} ) L^c_{Grad-CAM} = Relu(\\sum\\limits_{k} \\alpha^c_k A^k)","title":"Gradient Class Activation Map (Grad-CAM) Summary"},{"location":"MyBlogs/GradCam/#gradient-class-activation-map-grad-cam-summary","text":"The following note demonstrates the logic procedure of how a GradCam is computed. For details, please see: Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization","title":"Gradient Class Activation Map (Grad-CAM) Summary"},{"location":"MyBlogs/GradCam/#assumption","text":"","title":"Assumption"},{"location":"MyBlogs/GradCam/#class-activation-mapping","text":"y^{c} = \\sum\\limits_{k} w^k_c \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} A^k_{ij} The penultimate layer produces K K activation map A^K A^K with width u u and height v v . The feature goes through \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} , which is called global averaged pooled , and then linearly transforemd to y^{c} y^{c} for each class c c .","title":"Class Activation Mapping"},{"location":"MyBlogs/GradCam/#procedure","text":"","title":"Procedure"},{"location":"MyBlogs/GradCam/#1-choose-the-penultimate-layer","text":"This is the layer where we want to visualize the activation map. The reason why it is called the penultimate layer is because the last layer of a model is usually the layer computing class-specific loss (i.e. Sigmoid loss, softmax loss, etc). The penultimate layer is a layer, which is before the last layer, that we want to compute the loss respect to.","title":"1. Choose the penultimate layer."},{"location":"MyBlogs/GradCam/#2-compute-the-class-specific-loss-ie-loss-of-ycyc-on-the-last-layer-of-the-model","text":"","title":"2. Compute the class-specific loss (i.e. loss of y^{c}y^{c}) on the last layer of the model."},{"location":"MyBlogs/GradCam/#3-compute-the-gradients-of-ycyc-respect-to-the-penultimate-layers-activation-map-akak","text":"\\alpha^c_k = \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} \\frac{\\partial y^c}{\\partial A^k_{ij}} The dimension of the gradients is then reduced from (batch_size, u, v, channel) to (channel) by averaging the values in the first three axises. In the paper, this dimension reduction is called global-average-pooling . This result denoted as \\alpha^c_k \\alpha^c_k represents a partial linearization of the penultimate layer to the loss, which captures the importance of the activation A^k A^k for a target c c .","title":"3. Compute the gradients of y^cy^c respect to the penultimate layer's activation map A^KA^K."},{"location":"MyBlogs/GradCam/#4-elemently-wise-multpile-the-the-gradient-with-the-activation-map","text":"","title":"4. Elemently-wise multpile the the gradient with the Activation map."},{"location":"MyBlogs/GradCam/#5-remove-any-negative-results-by-applying-the-relu-function-over-the-result","text":"Only interested in the positive influence on the class of interest. (i.e. Pixels whose intensity should increase in order to increase y^{c} y^{c} ) L^c_{Grad-CAM} = Relu(\\sum\\limits_{k} \\alpha^c_k A^k)","title":"5. Remove any negative results by applying the Relu function over the result."},{"location":"MyBlogs/nf1/","text":"Normalizing Flows and Its Friends (Part 1) \u00b6 Here is some pre-requisite knowledge required before we move to Normalizing Flows. 1. Jacobian Matrix \u00b6 Jacobian is a just matrix containing all partial derivatives between each input and output. It looks scary, but in fact, it is just a fancy name for a partial derivative matrix. For example, if z = \\begin{bmatrix} z_{1} \\\\ z_{2} \\end{bmatrix} z = \\begin{bmatrix} z_{1} \\\\ z_{2} \\end{bmatrix} x = \\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix} x = \\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix} and z = f(x) z = f(x) x = f^{-1}(z) x = f^{-1}(z) then, J_{f} = \\begin{bmatrix} \\frac{\\partial x_{1}}{\\partial z_{1}} & \\frac{\\partial x_{1}}{\\partial z_{2}} \\\\ \\frac{\\partial x_{2}}{\\partial z_{1}} & \\frac{\\partial x_{2}}{\\partial z_{2}} \\end{bmatrix} J_{f} = \\begin{bmatrix} \\frac{\\partial x_{1}}{\\partial z_{1}} & \\frac{\\partial x_{1}}{\\partial z_{2}} \\\\ \\frac{\\partial x_{2}}{\\partial z_{1}} & \\frac{\\partial x_{2}}{\\partial z_{2}} \\end{bmatrix} J_{f^{-1}} = \\begin{bmatrix} \\frac{\\partial z_{1}}{\\partial x_{1}} & \\frac{\\partial z_{1}}{\\partial x_{2}} \\\\ \\frac{\\partial z_{2}}{\\partial x_{1}} & \\frac{\\partial z_{2}}{\\partial x_{2}} \\end{bmatrix} J_{f^{-1}} = \\begin{bmatrix} \\frac{\\partial z_{1}}{\\partial x_{1}} & \\frac{\\partial z_{1}}{\\partial x_{2}} \\\\ \\frac{\\partial z_{2}}{\\partial x_{1}} & \\frac{\\partial z_{2}}{\\partial x_{2}} \\end{bmatrix} J_{f} J_{f^{-1}} = I J_{f} J_{f^{-1}} = I 2. Determinant \u00b6 Determinant is a scalar value, which describes the area/volume enclosed by all the vectors in a matrix. For example, if A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} then, det(A) = ad - bc det(A) = ad - bc The most important thing is that: det(A) = {\\frac{1}{A^{-1}}} det(A) = {\\frac{1}{A^{-1}}} det(J_{f}) = {\\frac{1}{J_{f^{-1}}}} det(J_{f}) = {\\frac{1}{J_{f^{-1}}}} In a 2D space, it looks like this: 3. Change of Variable Theorem \u00b6 p(x') \\mid det {\\begin{bmatrix} \\Delta x_{11} & \\Delta x_{21} \\\\ \\Delta x_{12} & \\Delta x_{22} \\end{bmatrix}} \\mid = \\pi(z') \\Delta {z_{1}} \\Delta {z_{2}} p(x') \\mid det {\\begin{bmatrix} \\Delta x_{11} & \\Delta x_{21} \\\\ \\Delta x_{12} & \\Delta x_{22} \\end{bmatrix}} \\mid = \\pi(z') \\Delta {z_{1}} \\Delta {z_{2}} and this becomes to p(x') \\mid det(J_{f}) \\mid = \\pi(z') p(x') \\mid det(J_{f}) \\mid = \\pi(z') p(x') = \\pi(z') \\mid {det(J_{f^{-1}})} \\mid p(x') = \\pi(z') \\mid {det(J_{f^{-1}})} \\mid Referemces \u00b6 https://www.youtube.com/watch?v=uXY18nzdSsM&t=169s","title":"**Normalizing Flows and Its Friends (Part 1)**"},{"location":"MyBlogs/nf1/#normalizing-flows-and-its-friends-part-1","text":"Here is some pre-requisite knowledge required before we move to Normalizing Flows.","title":"Normalizing Flows and Its Friends (Part 1)"},{"location":"MyBlogs/nf1/#1-jacobian-matrix","text":"Jacobian is a just matrix containing all partial derivatives between each input and output. It looks scary, but in fact, it is just a fancy name for a partial derivative matrix. For example, if z = \\begin{bmatrix} z_{1} \\\\ z_{2} \\end{bmatrix} z = \\begin{bmatrix} z_{1} \\\\ z_{2} \\end{bmatrix} x = \\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix} x = \\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix} and z = f(x) z = f(x) x = f^{-1}(z) x = f^{-1}(z) then, J_{f} = \\begin{bmatrix} \\frac{\\partial x_{1}}{\\partial z_{1}} & \\frac{\\partial x_{1}}{\\partial z_{2}} \\\\ \\frac{\\partial x_{2}}{\\partial z_{1}} & \\frac{\\partial x_{2}}{\\partial z_{2}} \\end{bmatrix} J_{f} = \\begin{bmatrix} \\frac{\\partial x_{1}}{\\partial z_{1}} & \\frac{\\partial x_{1}}{\\partial z_{2}} \\\\ \\frac{\\partial x_{2}}{\\partial z_{1}} & \\frac{\\partial x_{2}}{\\partial z_{2}} \\end{bmatrix} J_{f^{-1}} = \\begin{bmatrix} \\frac{\\partial z_{1}}{\\partial x_{1}} & \\frac{\\partial z_{1}}{\\partial x_{2}} \\\\ \\frac{\\partial z_{2}}{\\partial x_{1}} & \\frac{\\partial z_{2}}{\\partial x_{2}} \\end{bmatrix} J_{f^{-1}} = \\begin{bmatrix} \\frac{\\partial z_{1}}{\\partial x_{1}} & \\frac{\\partial z_{1}}{\\partial x_{2}} \\\\ \\frac{\\partial z_{2}}{\\partial x_{1}} & \\frac{\\partial z_{2}}{\\partial x_{2}} \\end{bmatrix} J_{f} J_{f^{-1}} = I J_{f} J_{f^{-1}} = I","title":"1. Jacobian Matrix"},{"location":"MyBlogs/nf1/#2-determinant","text":"Determinant is a scalar value, which describes the area/volume enclosed by all the vectors in a matrix. For example, if A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} then, det(A) = ad - bc det(A) = ad - bc The most important thing is that: det(A) = {\\frac{1}{A^{-1}}} det(A) = {\\frac{1}{A^{-1}}} det(J_{f}) = {\\frac{1}{J_{f^{-1}}}} det(J_{f}) = {\\frac{1}{J_{f^{-1}}}} In a 2D space, it looks like this:","title":"2. Determinant"},{"location":"MyBlogs/nf1/#3-change-of-variable-theorem","text":"p(x') \\mid det {\\begin{bmatrix} \\Delta x_{11} & \\Delta x_{21} \\\\ \\Delta x_{12} & \\Delta x_{22} \\end{bmatrix}} \\mid = \\pi(z') \\Delta {z_{1}} \\Delta {z_{2}} p(x') \\mid det {\\begin{bmatrix} \\Delta x_{11} & \\Delta x_{21} \\\\ \\Delta x_{12} & \\Delta x_{22} \\end{bmatrix}} \\mid = \\pi(z') \\Delta {z_{1}} \\Delta {z_{2}} and this becomes to p(x') \\mid det(J_{f}) \\mid = \\pi(z') p(x') \\mid det(J_{f}) \\mid = \\pi(z') p(x') = \\pi(z') \\mid {det(J_{f^{-1}})} \\mid p(x') = \\pi(z') \\mid {det(J_{f^{-1}})} \\mid","title":"3. Change of Variable Theorem"},{"location":"MyBlogs/nf1/#referemces","text":"https://www.youtube.com/watch?v=uXY18nzdSsM&t=169s","title":"Referemces"},{"location":"MyBlogs/nf2/","text":"Normalizing Flows and Its Friends (Part 2) \u00b6 What is normalizing flows? \u00b6 One type of generative models is flow-based models, which explicitly learns the mapping between a group of samples x_{1} x_{1} , x_{2} x_{2} , ..., x_{n} x_{n} to their latent distribution p(x) p(x) . Normalizing flows is one example of flow-based models. Why normalizing flows? \u00b6 Given some real world data samples, we are always interested in learning its underlying distributions. Knowing the exact data distribution p(x) p(x) is helpful in some scenarios such as sampling data and identifying bias. Objective To minimize some notion of distance between p_{D} p_{D} and p_{M} p_{M} Given a dataset X=x_{1}, x_{2}, x_{3}, ... X=x_{1}, x_{2}, x_{3}, ... from an underlying distribution p_{D} p_{D} , Can we find an approximating distribution p_{M} p_{M} , which is from a family of M M and parametrized by \\theta \\theta , to minimize the distance between p_{D} p_{D} and p_{M} p_{M} ? Mathmatically, it is written as below \\theta^{*}=\\underset{\\theta \\in M}{\\mathrm{argmin}} \\space dist(p_{\\theta}, p_{D}$) \\theta^{*}=\\underset{\\theta \\in M}{\\mathrm{argmin}} \\space dist(p_{\\theta}, p_{D}$) If KL-divergence is the distance function, the above equation becomes the maximum likelihood estimation ... \\theta^{*}=\\underset{\\theta \\in M}{\\mathrm{argmin}} \\space E_{x~p_{D}} [-log \\space p_{\\theta}(x)] \\theta^{*}=\\underset{\\theta \\in M}{\\mathrm{argmin}} \\space E_{x~p_{D}} [-log \\space p_{\\theta}(x)] Flow-based models are different than GAN or VAE, since Flow-based models explicitly learn p(x) p(x) by optimizing the log likelihood. In GAN, the probability density function estimation is implicit by having the minmax classification error. We don't explicitly assign a probability density function and estimate it. In VAE, we get an approximate probability density function by optimizing the evidence lower bound (which is p_{\\theta}(x|z) p_{\\theta}(x|z) ). The encoder captures the approximate posterior mapping between x x and z z , which is q q parameterized by \\phi \\phi . The decoder captures p_{\\theta}(x|z) p_{\\theta}(x|z) which is parameterized by \\theta \\theta . In this casce, it is an approximate density estimation. In short, both GAN and VAE do not explicitly learn probability density function of real data p(x) p(x) . In flow-based models, given an x x , we want to find the function f f to get the latent representation z z . And if we invert f f , we will get x x back. The function f(x) f(x) and f^{-1}_(x) f^{-1}_(x) are exactly the inverse. And the flow-based models try to capture f f How does it work? \u00b6 We try to identify a transformation f: Z \\rightarrow X f: Z \\rightarrow X where f is a series of differentiable and invertible functions ( f_{1} f_{1} , f_{2} f_{2} , ..., f_{K} f_{K} ,) In general, for any invertible function f: Z \\rightarrow X f: Z \\rightarrow X , the probability function is below. The detailed steps can be found in this page p(x') = \\pi(z') \\mid {det(J_{f^{-1}})} \\mid p(x') = \\pi(z') \\mid {det(J_{f^{-1}})} \\mid log \\space p(x') = log \\space \\pi(z') + \\sum^{K}_{i=1} log \\mid {det(J_{f^{-1}})} \\mid log \\space p(x') = log \\space \\pi(z') + \\sum^{K}_{i=1} log \\mid {det(J_{f^{-1}})} \\mid Intuition: The first term describes the transformation f f moulds the density p_{Z}(z) p_{Z}(z) into p_{X}(x) p_{X}(x) . The second term describes the relative change of volume around z z In summary, the three requirements must hold for a normalizing flow model: Transformation function f f should be differentiable Transformation function f f should be invertible Determinant of Jacobian should be easy to compute Example 1: NICE (Non-linear Independent Components Estimation) \u00b6 Coupling layer operation: y_{1} = x_{1} y_{2} = g(x_{2};m(x_{1})) y_{2} = g(x_{2};m(x_{1})) Therefore, its Jacobian is a lower-triangular matrix and the determinant is the product of diagonal elements \\frac{\\partial y}{\\partial x} = \\begin{bmatrix} I & 0 \\\\ \\frac{\\partial y_{2}}{\\partial x_{1}} & \\frac{\\partial y_{2}}{\\partial x_{2}} \\end{bmatrix} \\frac{\\partial y}{\\partial x} = \\begin{bmatrix} I & 0 \\\\ \\frac{\\partial y_{2}}{\\partial x_{1}} & \\frac{\\partial y_{2}}{\\partial x_{2}} \\end{bmatrix} The inverse mappings are: x_{1} = y_{1} x_{1} = y_{1} x_{2} = g^{-1}(y_{2}, m(y_{1})) x_{2} = g^{-1}(y_{2}, m(y_{1})) Example 2: Real NVP (Real-valued Non Volume Preserving) \u00b6 Affine coupling operations are (there is one translation component and one scale component for y_{2} y_{2} ): y_{1} = x_{1} y_{1} = x_{1} y_{2} = x_{2} \\odot exp(s(x_{1})) + t(x_{1}) y_{2} = x_{2} \\odot exp(s(x_{1})) + t(x_{1}) The Jacobian becomes \\frac{\\partial y}{\\partial x} = \\begin{bmatrix} I_{d} & 0 \\\\ \\frac{\\partial y_{2}}{\\partial x_{1}} & diag(exp[s(x_{1})]) \\end{bmatrix} \\frac{\\partial y}{\\partial x} = \\begin{bmatrix} I_{d} & 0 \\\\ \\frac{\\partial y_{2}}{\\partial x_{1}} & diag(exp[s(x_{1})]) \\end{bmatrix} Since the Jacobian matrix is not always equal to 1, affine coupling is not always volume preserving which is more realistic in real world data. The inverse operations are: x_{1} = y_{1} x_{1} = y_{1} x_{2} = (y_{2} - t(y_{1})) \\odot exp(-s(y_{1})) x_{2} = (y_{2} - t(y_{1})) \\odot exp(-s(y_{1})) References \u00b6 https://www.youtube.com/watch?v=RPkf516rXgw https://www.youtube.com/watch?v=PCfHd0Ec6M4 https://stackoverflow.com/questions/54635355/what-does-log-prob-do https://www.youtube.com/watch?v=uXY18nzdSsM&t=169s","title":"**Normalizing Flows and Its Friends (Part 2)**"},{"location":"MyBlogs/nf2/#normalizing-flows-and-its-friends-part-2","text":"","title":"Normalizing Flows and Its Friends (Part 2)"},{"location":"MyBlogs/nf2/#what-is-normalizing-flows","text":"One type of generative models is flow-based models, which explicitly learns the mapping between a group of samples x_{1} x_{1} , x_{2} x_{2} , ..., x_{n} x_{n} to their latent distribution p(x) p(x) . Normalizing flows is one example of flow-based models.","title":"What is normalizing flows?"},{"location":"MyBlogs/nf2/#why-normalizing-flows","text":"Given some real world data samples, we are always interested in learning its underlying distributions. Knowing the exact data distribution p(x) p(x) is helpful in some scenarios such as sampling data and identifying bias. Objective To minimize some notion of distance between p_{D} p_{D} and p_{M} p_{M} Given a dataset X=x_{1}, x_{2}, x_{3}, ... X=x_{1}, x_{2}, x_{3}, ... from an underlying distribution p_{D} p_{D} , Can we find an approximating distribution p_{M} p_{M} , which is from a family of M M and parametrized by \\theta \\theta , to minimize the distance between p_{D} p_{D} and p_{M} p_{M} ? Mathmatically, it is written as below \\theta^{*}=\\underset{\\theta \\in M}{\\mathrm{argmin}} \\space dist(p_{\\theta}, p_{D}$) \\theta^{*}=\\underset{\\theta \\in M}{\\mathrm{argmin}} \\space dist(p_{\\theta}, p_{D}$) If KL-divergence is the distance function, the above equation becomes the maximum likelihood estimation ... \\theta^{*}=\\underset{\\theta \\in M}{\\mathrm{argmin}} \\space E_{x~p_{D}} [-log \\space p_{\\theta}(x)] \\theta^{*}=\\underset{\\theta \\in M}{\\mathrm{argmin}} \\space E_{x~p_{D}} [-log \\space p_{\\theta}(x)] Flow-based models are different than GAN or VAE, since Flow-based models explicitly learn p(x) p(x) by optimizing the log likelihood. In GAN, the probability density function estimation is implicit by having the minmax classification error. We don't explicitly assign a probability density function and estimate it. In VAE, we get an approximate probability density function by optimizing the evidence lower bound (which is p_{\\theta}(x|z) p_{\\theta}(x|z) ). The encoder captures the approximate posterior mapping between x x and z z , which is q q parameterized by \\phi \\phi . The decoder captures p_{\\theta}(x|z) p_{\\theta}(x|z) which is parameterized by \\theta \\theta . In this casce, it is an approximate density estimation. In short, both GAN and VAE do not explicitly learn probability density function of real data p(x) p(x) . In flow-based models, given an x x , we want to find the function f f to get the latent representation z z . And if we invert f f , we will get x x back. The function f(x) f(x) and f^{-1}_(x) f^{-1}_(x) are exactly the inverse. And the flow-based models try to capture f f","title":"Why normalizing flows?"},{"location":"MyBlogs/nf2/#how-does-it-work","text":"We try to identify a transformation f: Z \\rightarrow X f: Z \\rightarrow X where f is a series of differentiable and invertible functions ( f_{1} f_{1} , f_{2} f_{2} , ..., f_{K} f_{K} ,) In general, for any invertible function f: Z \\rightarrow X f: Z \\rightarrow X , the probability function is below. The detailed steps can be found in this page p(x') = \\pi(z') \\mid {det(J_{f^{-1}})} \\mid p(x') = \\pi(z') \\mid {det(J_{f^{-1}})} \\mid log \\space p(x') = log \\space \\pi(z') + \\sum^{K}_{i=1} log \\mid {det(J_{f^{-1}})} \\mid log \\space p(x') = log \\space \\pi(z') + \\sum^{K}_{i=1} log \\mid {det(J_{f^{-1}})} \\mid Intuition: The first term describes the transformation f f moulds the density p_{Z}(z) p_{Z}(z) into p_{X}(x) p_{X}(x) . The second term describes the relative change of volume around z z In summary, the three requirements must hold for a normalizing flow model: Transformation function f f should be differentiable Transformation function f f should be invertible Determinant of Jacobian should be easy to compute","title":"How does it work?"},{"location":"MyBlogs/nf2/#example-1-nice-non-linear-independent-components-estimation","text":"Coupling layer operation: y_{1} = x_{1} y_{2} = g(x_{2};m(x_{1})) y_{2} = g(x_{2};m(x_{1})) Therefore, its Jacobian is a lower-triangular matrix and the determinant is the product of diagonal elements \\frac{\\partial y}{\\partial x} = \\begin{bmatrix} I & 0 \\\\ \\frac{\\partial y_{2}}{\\partial x_{1}} & \\frac{\\partial y_{2}}{\\partial x_{2}} \\end{bmatrix} \\frac{\\partial y}{\\partial x} = \\begin{bmatrix} I & 0 \\\\ \\frac{\\partial y_{2}}{\\partial x_{1}} & \\frac{\\partial y_{2}}{\\partial x_{2}} \\end{bmatrix} The inverse mappings are: x_{1} = y_{1} x_{1} = y_{1} x_{2} = g^{-1}(y_{2}, m(y_{1})) x_{2} = g^{-1}(y_{2}, m(y_{1}))","title":"Example 1: NICE (Non-linear Independent Components Estimation)"},{"location":"MyBlogs/nf2/#example-2-real-nvp-real-valued-non-volume-preserving","text":"Affine coupling operations are (there is one translation component and one scale component for y_{2} y_{2} ): y_{1} = x_{1} y_{1} = x_{1} y_{2} = x_{2} \\odot exp(s(x_{1})) + t(x_{1}) y_{2} = x_{2} \\odot exp(s(x_{1})) + t(x_{1}) The Jacobian becomes \\frac{\\partial y}{\\partial x} = \\begin{bmatrix} I_{d} & 0 \\\\ \\frac{\\partial y_{2}}{\\partial x_{1}} & diag(exp[s(x_{1})]) \\end{bmatrix} \\frac{\\partial y}{\\partial x} = \\begin{bmatrix} I_{d} & 0 \\\\ \\frac{\\partial y_{2}}{\\partial x_{1}} & diag(exp[s(x_{1})]) \\end{bmatrix} Since the Jacobian matrix is not always equal to 1, affine coupling is not always volume preserving which is more realistic in real world data. The inverse operations are: x_{1} = y_{1} x_{1} = y_{1} x_{2} = (y_{2} - t(y_{1})) \\odot exp(-s(y_{1})) x_{2} = (y_{2} - t(y_{1})) \\odot exp(-s(y_{1}))","title":"Example 2: Real NVP (Real-valued Non Volume Preserving)"},{"location":"MyBlogs/nf2/#references","text":"https://www.youtube.com/watch?v=RPkf516rXgw https://www.youtube.com/watch?v=PCfHd0Ec6M4 https://stackoverflow.com/questions/54635355/what-does-log-prob-do https://www.youtube.com/watch?v=uXY18nzdSsM&t=169s","title":"References"},{"location":"MyBlogs/rle/","text":"","title":"Rle"}]}