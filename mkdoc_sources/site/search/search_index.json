{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Story of My Kingdom! \u00b6 img { border: 1px solid #ddd; border-radius: 4px; padding: 5px; width: 640px; float: center } My name is Liqiang Ding. I am a cat-lover, a big fun of sci-fi movies and classical music. This blog is a place to share my discoveries from life and work. I wish you enjoy reading it. :)","title":"Home"},{"location":"#welcome-to-the-story-of-my-kingdom","text":"img { border: 1px solid #ddd; border-radius: 4px; padding: 5px; width: 640px; float: center } My name is Liqiang Ding. I am a cat-lover, a big fun of sci-fi movies and classical music. This blog is a place to share my discoveries from life and work. I wish you enjoy reading it. :)","title":"Welcome to the Story of My Kingdom!"},{"location":"about/","text":"About Myself \u00b6 Artificial Intelligence? Computer Vision? Natural language Processing? \u00b6 Artificial Intelligence never fails to appeal to me. Just like Doraemon to Nobita Nobi , I have been dreaming of having a computer friend since I was a kid. It has a bunch of fancy inventions from the 22 nd century in his four dimension pocket, joins me on the adventure exploring this world, and most importantly, teaches by honest words and actions instead of banalities. For years, I scratched numerous drafts about how it would look like, imagined the fancy capabilities it might have, and even came up with scenarios where it could come to realities without scaring my parents. Well, most of them is lost in time, but the fantasy for building such a powerful program still sticks to my mind. Finished the elementary school and high school in China, I was fortunate to came aboard to Canada to continue my study at McGill University, majoring in eletrical engineering. Studying at McGill really broadened my horizons, and it was the real starting point for me to get to know programming algorithms, electrical circuits, information theory and a lot of cool stuff. Being exposure to this massive amount of new knowledge is an exciting yet challenging journey, and no need for mentioning how many nights spent in the Schulich library. However, there was always one question confusing me: What do I want to do with these great pieces of knowledge? Trying to find this answer, I took a few internships across different industries, with various technical focuses. I witnessed how teams across the world leveraged their own experties, contributing to making a great product that faciliated millions of people. I was also amazed to see that people were willing to spend days in improving browsers just to render contents with 1 second faster. On one side, all of these experiences pushed me to go further with my study, and opened completely new worlds. On the other side, they also deepened my question. Time is limited, decisions need to be made. In the last year of my bachelor study, one of my friends mentioned me a course named computer vision that he was about to take. For curiosity, I clicked the course website in the evening, and was greatly surprised of the amazing things that it can do. For the first time, I realized computer algorithms are not only applied to sort an array or to design the shortest path for a busy postman. Instead, it can be so connected to the most of vision-related tasks we perform in the daily life, to recognize a poker card, to restore an old image, and even to stabilize poor-taken videos from amateurs. I took the course without hesitation. It was also around the same time when everyone started to talk deep learning. The success of AlexNet proved that applying the data-hungry iterative process is feasible even in the high dimensional data like images. Millions of dollars were spent by the those tech giants to build AI labs. The power of supervised learning can even be explained clearly by hosts in late night TV shows\u3002 It looked like everything could be solved by deep learning, and the only problem remained as how we can apply this fashionable approach to actually do them. Being fascinated to know all of this, I thought it would be cool to go one step further by taking the graduate school, in order to have a better understanding of how things exactly work. With a mixed feeling for being excited and concerned, I chose the graduate school at McGill over a job offer at a bank. Many years later when I looked back at this decision in those sleepless nights, I really appreciated the choice I made: It is always easier to believe a plausible statement from others, rather than figuring out if the statement is really valid by myself. The Deep learning, which sounds promising and does deliver astonishing performances, also comes with a lot of non-neglectable problems. Through my graduate study, one of my main tasks was to identify these limitations and designed corresponding solutions to mitigate them.","title":"About Myself"},{"location":"about/#about-myself","text":"","title":"About Myself"},{"location":"about/#artificial-intelligence-computer-vision-natural-language-processing","text":"Artificial Intelligence never fails to appeal to me. Just like Doraemon to Nobita Nobi , I have been dreaming of having a computer friend since I was a kid. It has a bunch of fancy inventions from the 22 nd century in his four dimension pocket, joins me on the adventure exploring this world, and most importantly, teaches by honest words and actions instead of banalities. For years, I scratched numerous drafts about how it would look like, imagined the fancy capabilities it might have, and even came up with scenarios where it could come to realities without scaring my parents. Well, most of them is lost in time, but the fantasy for building such a powerful program still sticks to my mind. Finished the elementary school and high school in China, I was fortunate to came aboard to Canada to continue my study at McGill University, majoring in eletrical engineering. Studying at McGill really broadened my horizons, and it was the real starting point for me to get to know programming algorithms, electrical circuits, information theory and a lot of cool stuff. Being exposure to this massive amount of new knowledge is an exciting yet challenging journey, and no need for mentioning how many nights spent in the Schulich library. However, there was always one question confusing me: What do I want to do with these great pieces of knowledge? Trying to find this answer, I took a few internships across different industries, with various technical focuses. I witnessed how teams across the world leveraged their own experties, contributing to making a great product that faciliated millions of people. I was also amazed to see that people were willing to spend days in improving browsers just to render contents with 1 second faster. On one side, all of these experiences pushed me to go further with my study, and opened completely new worlds. On the other side, they also deepened my question. Time is limited, decisions need to be made. In the last year of my bachelor study, one of my friends mentioned me a course named computer vision that he was about to take. For curiosity, I clicked the course website in the evening, and was greatly surprised of the amazing things that it can do. For the first time, I realized computer algorithms are not only applied to sort an array or to design the shortest path for a busy postman. Instead, it can be so connected to the most of vision-related tasks we perform in the daily life, to recognize a poker card, to restore an old image, and even to stabilize poor-taken videos from amateurs. I took the course without hesitation. It was also around the same time when everyone started to talk deep learning. The success of AlexNet proved that applying the data-hungry iterative process is feasible even in the high dimensional data like images. Millions of dollars were spent by the those tech giants to build AI labs. The power of supervised learning can even be explained clearly by hosts in late night TV shows\u3002 It looked like everything could be solved by deep learning, and the only problem remained as how we can apply this fashionable approach to actually do them. Being fascinated to know all of this, I thought it would be cool to go one step further by taking the graduate school, in order to have a better understanding of how things exactly work. With a mixed feeling for being excited and concerned, I chose the graduate school at McGill over a job offer at a bank. Many years later when I looked back at this decision in those sleepless nights, I really appreciated the choice I made: It is always easier to believe a plausible statement from others, rather than figuring out if the statement is really valid by myself. The Deep learning, which sounds promising and does deliver astonishing performances, also comes with a lot of non-neglectable problems. Through my graduate study, one of my main tasks was to identify these limitations and designed corresponding solutions to mitigate them.","title":"Artificial Intelligence? Computer Vision? Natural language Processing?"},{"location":"backup/","text":"","title":"Backup"},{"location":"blogs/","text":"Paper Reading: Human Pose Regression with Residual Log-likelihood Estimation \u00b6 Keywords Human pose estimation, laplace, gaussian priors Full Articale Normalizing Flows and Its Friends (Part 2) \u00b6 Keywords flow-based model, RealNVP Full Articale Normalizing Flows and Its Friends (Part 1) \u00b6 Keywords Jacobian Matrix, Determinant, Change of Variable Theorem Full Articale Gradient Class Activation Map (Grad-CAM) Summary \u00b6 Keywords GradCam, penultimate layer Full Articale","title":"Technical Blogs"},{"location":"blogs/#paper-reading-human-pose-regression-with-residual-log-likelihood-estimation","text":"Keywords Human pose estimation, laplace, gaussian priors Full Articale","title":"Paper Reading: Human Pose Regression with Residual Log-likelihood Estimation"},{"location":"blogs/#normalizing-flows-and-its-friends-part-2","text":"Keywords flow-based model, RealNVP Full Articale","title":"Normalizing Flows and Its Friends (Part 2)"},{"location":"blogs/#normalizing-flows-and-its-friends-part-1","text":"Keywords Jacobian Matrix, Determinant, Change of Variable Theorem Full Articale","title":"Normalizing Flows and Its Friends (Part 1)"},{"location":"blogs/#gradient-class-activation-map-grad-cam-summary","text":"Keywords GradCam, penultimate layer Full Articale","title":"Gradient Class Activation Map (Grad-CAM) Summary"},{"location":"gallery/","text":"Under construction. Coming Soon...","title":"Gallery"},{"location":"listen_and_learn/","text":"Learning is a life-long process ... \u00b6 ML Operations Generative Adversariel Networks Certificate Generative Adversariel Networks Course 1 Generative Adversariel Networks Course 2 Generative Adversariel Networks Course 3 Sequence Models Convolutional Neural Networks Probabilistic Graphics Model","title":"Listen and Learn"},{"location":"listen_and_learn/#learning-is-a-life-long-process","text":"ML Operations Generative Adversariel Networks Certificate Generative Adversariel Networks Course 1 Generative Adversariel Networks Course 2 Generative Adversariel Networks Course 3 Sequence Models Convolutional Neural Networks Probabilistic Graphics Model","title":"Learning is a life-long process ..."},{"location":"projects/","text":"PythonWoW \u00b6 As a machine learning developer/researcher, I encountered many python related questions from time to time. The majority of them are straightforward, but some are subtle to understand, and hard to be applied properly in real practise. During the process for searching answers, I realized that it usually becomes super clear if I write them down with my own examples and understanding. This repository serves as my notes to track of these questions. I usually start from the problem statement raised from a real world example, and goes to the reasons behind it. I would be also very happy if this can help you. Click me for details Tensorflow2 Practises \u00b6 Finally, tensorflow got upgraded to 2.x. It is time to try it out and have some fun :) Click me for details Code practise with Efficient Python \u00b6 This repository contains notes that I took while reading this book. In general, this is a very fruitful journey. The author covers eight major areas about python programming, which are super interesting and especially helpful for my daily work as a machine learning engineer. For these intermediate/senior level technical books, I find that the best way to absorb them is to code as I read them through. Therefore, all the codes in this repo are the examples that the author mentions in his book. Also, since a number of chapters require some additional background knowledge, I also write them down together with my own interpretation in separate files, which live in the relevant folders. Anyway, I will be very glad if you find these examples are helpful for your projects, or you just have some fun reading them. Please feel free to send me an email if you find a mistake or you don't agree with my interpretations. Click me for details Frequency \u00b6 A video receiver with analog circuits. Wait till the end for an easter egg (if you are big a fun of the movie Inception ) When Beethoven Meets VHDL \u00b6 A music box with digital circuits. McGill Gear Solid \u00b6 Led a team of 6 from different engineering faculties to design a robot in 7 weeks. The robot is able to localize, navigate, avoid obstacles and rescue blocks on 365*365 cm maps by using a single processor and sensors","title":"Personal Projects"},{"location":"projects/#pythonwow","text":"As a machine learning developer/researcher, I encountered many python related questions from time to time. The majority of them are straightforward, but some are subtle to understand, and hard to be applied properly in real practise. During the process for searching answers, I realized that it usually becomes super clear if I write them down with my own examples and understanding. This repository serves as my notes to track of these questions. I usually start from the problem statement raised from a real world example, and goes to the reasons behind it. I would be also very happy if this can help you. Click me for details","title":"PythonWoW"},{"location":"projects/#tensorflow2-practises","text":"Finally, tensorflow got upgraded to 2.x. It is time to try it out and have some fun :) Click me for details","title":"Tensorflow2 Practises"},{"location":"projects/#code-practise-with-efficient-python","text":"This repository contains notes that I took while reading this book. In general, this is a very fruitful journey. The author covers eight major areas about python programming, which are super interesting and especially helpful for my daily work as a machine learning engineer. For these intermediate/senior level technical books, I find that the best way to absorb them is to code as I read them through. Therefore, all the codes in this repo are the examples that the author mentions in his book. Also, since a number of chapters require some additional background knowledge, I also write them down together with my own interpretation in separate files, which live in the relevant folders. Anyway, I will be very glad if you find these examples are helpful for your projects, or you just have some fun reading them. Please feel free to send me an email if you find a mistake or you don't agree with my interpretations. Click me for details","title":"Code practise with Efficient Python"},{"location":"projects/#frequency","text":"A video receiver with analog circuits. Wait till the end for an easter egg (if you are big a fun of the movie Inception )","title":"Frequency"},{"location":"projects/#when-beethoven-meets-vhdl","text":"A music box with digital circuits.","title":"When Beethoven Meets VHDL"},{"location":"projects/#mcgill-gear-solid","text":"Led a team of 6 from different engineering faculties to design a robot in 7 weeks. The robot is able to localize, navigate, avoid obstacles and rescue blocks on 365*365 cm maps by using a single processor and sensors","title":"McGill Gear Solid"},{"location":"pub/","text":"My Research Interests \u00b6 My research focuses on representation learning with 3D data, self-supervised learning, few-shot learning, and domain adaption. Google scholar Task Adaptive Metric Space for Medium-Shot Medical Image Classification \u00b6 22th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2019 Full paper poster Artificial Intelligence for Real-Time Multiple Polyp Detection with Identification, Tracking, and Optical Biopsy During Colonoscopy \u00b6 Journal of Gastroenterology, 2019 Full paper A Single Framework for Domain Adaptation and Generalization in Medical Image Analysis \u00b6 Montreal AI Symposium (MAIS), 2018 Full paper poster Apparatus and method for detecting, classifying and tracking road users on frames of video data \u00b6 US Patent, Application US16/535,547 events Full description Deep-learning-based multiple object tracking in traffic surveillance video \u00b6 Master Thesis, McGill University, 2018 Full paper Generation of spatial-temporal panoramas with a single moving camera \u00b6 13 th Conference on Computer and Robot Vision (CRV), 2016 Full paper Video","title":"Research & Publication"},{"location":"pub/#my-research-interests","text":"My research focuses on representation learning with 3D data, self-supervised learning, few-shot learning, and domain adaption. Google scholar","title":"My Research Interests"},{"location":"pub/#task-adaptive-metric-space-for-medium-shot-medical-image-classification","text":"22th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2019 Full paper poster","title":"Task Adaptive Metric Space for Medium-Shot Medical Image Classification"},{"location":"pub/#artificial-intelligence-for-real-time-multiple-polyp-detection-with-identification-tracking-and-optical-biopsy-during-colonoscopy","text":"Journal of Gastroenterology, 2019 Full paper","title":"Artificial Intelligence for Real-Time Multiple Polyp Detection with Identification, Tracking, and Optical Biopsy During Colonoscopy"},{"location":"pub/#a-single-framework-for-domain-adaptation-and-generalization-in-medical-image-analysis","text":"Montreal AI Symposium (MAIS), 2018 Full paper poster","title":"A Single Framework for Domain Adaptation and Generalization in Medical Image Analysis"},{"location":"pub/#apparatus-and-method-for-detecting-classifying-and-tracking-road-users-on-frames-of-video-data","text":"US Patent, Application US16/535,547 events Full description","title":"Apparatus and method for detecting, classifying and tracking road users on frames of video data"},{"location":"pub/#deep-learning-based-multiple-object-tracking-in-traffic-surveillance-video","text":"Master Thesis, McGill University, 2018 Full paper","title":"Deep-learning-based multiple object tracking in traffic surveillance video"},{"location":"pub/#generation-of-spatial-temporal-panoramas-with-a-single-moving-camera","text":"13 th Conference on Computer and Robot Vision (CRV), 2016 Full paper Video","title":"Generation of spatial-temporal panoramas with a single moving camera"},{"location":"read/","text":"","title":"Read"},{"location":"MyBlogs/GradCam/","text":"Gradient Class Activation Map (Grad-CAM) Summary \u00b6 The following note demonstrates the logic procedure of how a GradCam is computed. For details, please see: Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization Assumption \u00b6 Class Activation Mapping \u00b6 y^{c} = \\sum\\limits_{k} w^k_c \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} A^k_{ij} The penultimate layer produces K K activation map A^K A^K with width u u and height v v . The feature goes through \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} , which is called global averaged pooled , and then linearly transforemd to y^{c} y^{c} for each class c c . Procedure \u00b6 1. Choose the penultimate layer. \u00b6 This is the layer where we want to visualize the activation map. The reason why it is called the penultimate layer is because the last layer of a model is usually the layer computing class-specific loss (i.e. Sigmoid loss, softmax loss, etc). The penultimate layer is a layer, which is before the last layer, that we want to compute the loss respect to. 2. Compute the class-specific loss (i.e. loss of y^{c} y^{c} ) on the last layer of the model. \u00b6 3. Compute the gradients of y^c y^c respect to the penultimate layer's activation map A^K A^K . \u00b6 \\alpha^c_k = \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} \\frac{\\partial y^c}{\\partial A^k_{ij}} The dimension of the gradients is then reduced from (batch_size, u, v, channel) to (channel) by averaging the values in the first three axises. In the paper, this dimension reduction is called global-average-pooling . This result denoted as \\alpha^c_k \\alpha^c_k represents a partial linearization of the penultimate layer to the loss, which captures the importance of the activation A^k A^k for a target c c . 4. Elemently-wise multpile the the gradient with the Activation map. \u00b6 5. Remove any negative results by applying the Relu function over the result. \u00b6 Only interested in the positive influence on the class of interest. (i.e. Pixels whose intensity should increase in order to increase y^{c} y^{c} ) L^c_{Grad-CAM} = Relu(\\sum\\limits_{k} \\alpha^c_k A^k)","title":"Gradient Class Activation Map (Grad-CAM) Summary"},{"location":"MyBlogs/GradCam/#gradient-class-activation-map-grad-cam-summary","text":"The following note demonstrates the logic procedure of how a GradCam is computed. For details, please see: Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization","title":"Gradient Class Activation Map (Grad-CAM) Summary"},{"location":"MyBlogs/GradCam/#assumption","text":"","title":"Assumption"},{"location":"MyBlogs/GradCam/#class-activation-mapping","text":"y^{c} = \\sum\\limits_{k} w^k_c \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} A^k_{ij} The penultimate layer produces K K activation map A^K A^K with width u u and height v v . The feature goes through \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} , which is called global averaged pooled , and then linearly transforemd to y^{c} y^{c} for each class c c .","title":"Class Activation Mapping"},{"location":"MyBlogs/GradCam/#procedure","text":"","title":"Procedure"},{"location":"MyBlogs/GradCam/#1-choose-the-penultimate-layer","text":"This is the layer where we want to visualize the activation map. The reason why it is called the penultimate layer is because the last layer of a model is usually the layer computing class-specific loss (i.e. Sigmoid loss, softmax loss, etc). The penultimate layer is a layer, which is before the last layer, that we want to compute the loss respect to.","title":"1. Choose the penultimate layer."},{"location":"MyBlogs/GradCam/#2-compute-the-class-specific-loss-ie-loss-of-ycyc-on-the-last-layer-of-the-model","text":"","title":"2. Compute the class-specific loss (i.e. loss of y^{c}y^{c}) on the last layer of the model."},{"location":"MyBlogs/GradCam/#3-compute-the-gradients-of-ycyc-respect-to-the-penultimate-layers-activation-map-akak","text":"\\alpha^c_k = \\frac{1}{Z} \\sum\\limits_{i} \\sum\\limits_{j} \\frac{\\partial y^c}{\\partial A^k_{ij}} The dimension of the gradients is then reduced from (batch_size, u, v, channel) to (channel) by averaging the values in the first three axises. In the paper, this dimension reduction is called global-average-pooling . This result denoted as \\alpha^c_k \\alpha^c_k represents a partial linearization of the penultimate layer to the loss, which captures the importance of the activation A^k A^k for a target c c .","title":"3. Compute the gradients of y^cy^c respect to the penultimate layer's activation map A^KA^K."},{"location":"MyBlogs/GradCam/#4-elemently-wise-multpile-the-the-gradient-with-the-activation-map","text":"","title":"4. Elemently-wise multpile the the gradient with the Activation map."},{"location":"MyBlogs/GradCam/#5-remove-any-negative-results-by-applying-the-relu-function-over-the-result","text":"Only interested in the positive influence on the class of interest. (i.e. Pixels whose intensity should increase in order to increase y^{c} y^{c} ) L^c_{Grad-CAM} = Relu(\\sum\\limits_{k} \\alpha^c_k A^k)","title":"5. Remove any negative results by applying the Relu function over the result."},{"location":"MyBlogs/nf1/","text":"Normalizing Flows and Its Friends (Part 1) \u00b6 Here is some pre-requisite knowledge required before we move to Normalizing Flows. 1. Jacobian Matrix \u00b6 Jacobian is a just matrix containing all partial derivatives between each input and output. It looks scary, but in fact, it is just a fancy name for a partial derivative matrix. For example, if z = \\begin{bmatrix} z_{1} \\\\ z_{2} \\end{bmatrix} z = \\begin{bmatrix} z_{1} \\\\ z_{2} \\end{bmatrix} x = \\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix} x = \\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix} and z = f(x) z = f(x) x = f^{-1}(z) x = f^{-1}(z) then, J_{f} = \\begin{bmatrix} \\frac{\\partial x_{1}}{\\partial z_{1}} & \\frac{\\partial x_{1}}{\\partial z_{2}} \\\\ \\frac{\\partial x_{2}}{\\partial z_{1}} & \\frac{\\partial x_{2}}{\\partial z_{2}} \\end{bmatrix} J_{f} = \\begin{bmatrix} \\frac{\\partial x_{1}}{\\partial z_{1}} & \\frac{\\partial x_{1}}{\\partial z_{2}} \\\\ \\frac{\\partial x_{2}}{\\partial z_{1}} & \\frac{\\partial x_{2}}{\\partial z_{2}} \\end{bmatrix} J_{f^{-1}} = \\begin{bmatrix} \\frac{\\partial z_{1}}{\\partial x_{1}} & \\frac{\\partial z_{1}}{\\partial x_{2}} \\\\ \\frac{\\partial z_{2}}{\\partial x_{1}} & \\frac{\\partial z_{2}}{\\partial x_{2}} \\end{bmatrix} J_{f^{-1}} = \\begin{bmatrix} \\frac{\\partial z_{1}}{\\partial x_{1}} & \\frac{\\partial z_{1}}{\\partial x_{2}} \\\\ \\frac{\\partial z_{2}}{\\partial x_{1}} & \\frac{\\partial z_{2}}{\\partial x_{2}} \\end{bmatrix} J_{f} J_{f^{-1}} = I J_{f} J_{f^{-1}} = I 2. Determinant \u00b6 Determinant is a scalar value, which describes the area/volume enclosed by all the vectors in a matrix. For example, if A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} then, det(A) = ad - bc det(A) = ad - bc The most important thing is that: det(A) = {\\frac{1}{A^{-1}}} det(A) = {\\frac{1}{A^{-1}}} det(J_{f}) = {\\frac{1}{J_{f^{-1}}}} det(J_{f}) = {\\frac{1}{J_{f^{-1}}}} In a 2D space, it looks like this: 3. Change of Variable Theorem \u00b6 p(x') \\mid det {\\begin{bmatrix} \\Delta x_{11} & \\Delta x_{21} \\\\ \\Delta x_{12} & \\Delta x_{22} \\end{bmatrix}} \\mid = \\pi(z') \\Delta {z_{1}} \\Delta {z_{2}} p(x') \\mid det {\\begin{bmatrix} \\Delta x_{11} & \\Delta x_{21} \\\\ \\Delta x_{12} & \\Delta x_{22} \\end{bmatrix}} \\mid = \\pi(z') \\Delta {z_{1}} \\Delta {z_{2}} and this becomes to p(x') \\mid det(J_{f}) \\mid = \\pi(z') p(x') \\mid det(J_{f}) \\mid = \\pi(z') p(x') = \\pi(z') \\mid {det(J_{f^{-1}})} \\mid p(x') = \\pi(z') \\mid {det(J_{f^{-1}})} \\mid Referemces \u00b6 https://www.youtube.com/watch?v=uXY18nzdSsM&t=169s","title":"**Normalizing Flows and Its Friends (Part 1)**"},{"location":"MyBlogs/nf1/#normalizing-flows-and-its-friends-part-1","text":"Here is some pre-requisite knowledge required before we move to Normalizing Flows.","title":"Normalizing Flows and Its Friends (Part 1)"},{"location":"MyBlogs/nf1/#1-jacobian-matrix","text":"Jacobian is a just matrix containing all partial derivatives between each input and output. It looks scary, but in fact, it is just a fancy name for a partial derivative matrix. For example, if z = \\begin{bmatrix} z_{1} \\\\ z_{2} \\end{bmatrix} z = \\begin{bmatrix} z_{1} \\\\ z_{2} \\end{bmatrix} x = \\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix} x = \\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix} and z = f(x) z = f(x) x = f^{-1}(z) x = f^{-1}(z) then, J_{f} = \\begin{bmatrix} \\frac{\\partial x_{1}}{\\partial z_{1}} & \\frac{\\partial x_{1}}{\\partial z_{2}} \\\\ \\frac{\\partial x_{2}}{\\partial z_{1}} & \\frac{\\partial x_{2}}{\\partial z_{2}} \\end{bmatrix} J_{f} = \\begin{bmatrix} \\frac{\\partial x_{1}}{\\partial z_{1}} & \\frac{\\partial x_{1}}{\\partial z_{2}} \\\\ \\frac{\\partial x_{2}}{\\partial z_{1}} & \\frac{\\partial x_{2}}{\\partial z_{2}} \\end{bmatrix} J_{f^{-1}} = \\begin{bmatrix} \\frac{\\partial z_{1}}{\\partial x_{1}} & \\frac{\\partial z_{1}}{\\partial x_{2}} \\\\ \\frac{\\partial z_{2}}{\\partial x_{1}} & \\frac{\\partial z_{2}}{\\partial x_{2}} \\end{bmatrix} J_{f^{-1}} = \\begin{bmatrix} \\frac{\\partial z_{1}}{\\partial x_{1}} & \\frac{\\partial z_{1}}{\\partial x_{2}} \\\\ \\frac{\\partial z_{2}}{\\partial x_{1}} & \\frac{\\partial z_{2}}{\\partial x_{2}} \\end{bmatrix} J_{f} J_{f^{-1}} = I J_{f} J_{f^{-1}} = I","title":"1. Jacobian Matrix"},{"location":"MyBlogs/nf1/#2-determinant","text":"Determinant is a scalar value, which describes the area/volume enclosed by all the vectors in a matrix. For example, if A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} then, det(A) = ad - bc det(A) = ad - bc The most important thing is that: det(A) = {\\frac{1}{A^{-1}}} det(A) = {\\frac{1}{A^{-1}}} det(J_{f}) = {\\frac{1}{J_{f^{-1}}}} det(J_{f}) = {\\frac{1}{J_{f^{-1}}}} In a 2D space, it looks like this:","title":"2. Determinant"},{"location":"MyBlogs/nf1/#3-change-of-variable-theorem","text":"p(x') \\mid det {\\begin{bmatrix} \\Delta x_{11} & \\Delta x_{21} \\\\ \\Delta x_{12} & \\Delta x_{22} \\end{bmatrix}} \\mid = \\pi(z') \\Delta {z_{1}} \\Delta {z_{2}} p(x') \\mid det {\\begin{bmatrix} \\Delta x_{11} & \\Delta x_{21} \\\\ \\Delta x_{12} & \\Delta x_{22} \\end{bmatrix}} \\mid = \\pi(z') \\Delta {z_{1}} \\Delta {z_{2}} and this becomes to p(x') \\mid det(J_{f}) \\mid = \\pi(z') p(x') \\mid det(J_{f}) \\mid = \\pi(z') p(x') = \\pi(z') \\mid {det(J_{f^{-1}})} \\mid p(x') = \\pi(z') \\mid {det(J_{f^{-1}})} \\mid","title":"3. Change of Variable Theorem"},{"location":"MyBlogs/nf1/#referemces","text":"https://www.youtube.com/watch?v=uXY18nzdSsM&t=169s","title":"Referemces"},{"location":"MyBlogs/nf2/","text":"Normalizing Flows and Its Friends (Part 2) \u00b6 What is normalizing flows? \u00b6 One type of generative models is flow-based models, which explicitly learns the mapping between a group of samples x_{1} x_{1} , x_{2} x_{2} , ..., x_{n} x_{n} to their latent distribution p(x) p(x) . Normalizing flows is one example of flow-based models. Why normalizing flows? \u00b6 Given some real world data samples, we are always interested in learning its underlying distributions. Knowing the exact data distribution p(x) p(x) is helpful in some scenarios such as sampling data and identifying bias. Objective To minimize some notion of distance between p_{D} p_{D} and p_{M} p_{M} Given a dataset X=x_{1}, x_{2}, x_{3}, ... X=x_{1}, x_{2}, x_{3}, ... from an underlying distribution p_{D} p_{D} , Can we find an approximating distribution p_{M} p_{M} , which is from a family of M M and parametrized by \\theta \\theta , to minimize the distance between p_{D} p_{D} and p_{M} p_{M} ? Mathmatically, it is written as below \\theta^{*}=\\underset{\\theta \\in M}{\\mathrm{argmin}} \\space dist(p_{\\theta}, p_{D}$) \\theta^{*}=\\underset{\\theta \\in M}{\\mathrm{argmin}} \\space dist(p_{\\theta}, p_{D}$) If KL-divergence is the distance function, the above equation becomes the maximum likelihood estimation ... \\theta^{*}=\\underset{\\theta \\in M}{\\mathrm{argmin}} \\space E_{x~p_{D}} [-log \\space p_{\\theta}(x)] \\theta^{*}=\\underset{\\theta \\in M}{\\mathrm{argmin}} \\space E_{x~p_{D}} [-log \\space p_{\\theta}(x)] Flow-based models are different than GAN or VAE, since Flow-based models explicitly learn p(x) p(x) by optimizing the log likelihood. In GAN, the probability density function estimation is implicit by having the minmax classification error. We don't explicitly assign a probability density function and estimate it. In VAE, we get an approximate probability density function by optimizing the evidence lower bound (which is p_{\\theta}(x|z) p_{\\theta}(x|z) ). The encoder captures the approximate posterior mapping between x x and z z , which is q q parameterized by \\phi \\phi . The decoder captures p_{\\theta}(x|z) p_{\\theta}(x|z) which is parameterized by \\theta \\theta . In this casce, it is an approximate density estimation. In short, both GAN and VAE do not explicitly learn probability density function of real data p(x) p(x) . In flow-based models, given an x x , we want to find the function f f to get the latent representation z z . And if we invert f f , we will get x x back. The function f(x) f(x) and f^{-1}_(x) f^{-1}_(x) are exactly the inverse. And the flow-based models try to capture f f How does it work? \u00b6 We try to identify a transformation f: Z \\rightarrow X f: Z \\rightarrow X where f is a series of differentiable and invertible functions ( f_{1} f_{1} , f_{2} f_{2} , ..., f_{K} f_{K} ,) In general, for any invertible function f: Z \\rightarrow X f: Z \\rightarrow X , the probability function is below. The detailed steps can be found in this page p(x') = \\pi(z') \\mid {det(J_{f^{-1}})} \\mid p(x') = \\pi(z') \\mid {det(J_{f^{-1}})} \\mid log \\space p(x') = log \\space \\pi(z') + \\sum^{K}_{i=1} log \\mid {det(J_{f^{-1}})} \\mid log \\space p(x') = log \\space \\pi(z') + \\sum^{K}_{i=1} log \\mid {det(J_{f^{-1}})} \\mid Intuition: The first term describes the transformation f f moulds the density p_{Z}(z) p_{Z}(z) into p_{X}(x) p_{X}(x) . The second term describes the relative change of volume around z z In summary, the three requirements must hold for a normalizing flow model: Transformation function f f should be differentiable Transformation function f f should be invertible Determinant of Jacobian should be easy to compute Example 1: NICE (Non-linear Independent Components Estimation) \u00b6 Coupling layer operation: y_{1} = x_{1} y_{2} = g(x_{2};m(x_{1})) y_{2} = g(x_{2};m(x_{1})) Therefore, its Jacobian is a lower-triangular matrix and the determinant is the product of diagonal elements \\frac{\\partial y}{\\partial x} = \\begin{bmatrix} I & 0 \\\\ \\frac{\\partial y_{2}}{\\partial x_{1}} & \\frac{\\partial y_{2}}{\\partial x_{2}} \\end{bmatrix} \\frac{\\partial y}{\\partial x} = \\begin{bmatrix} I & 0 \\\\ \\frac{\\partial y_{2}}{\\partial x_{1}} & \\frac{\\partial y_{2}}{\\partial x_{2}} \\end{bmatrix} The inverse mappings are: x_{1} = y_{1} x_{1} = y_{1} x_{2} = g^{-1}(y_{2}, m(y_{1})) x_{2} = g^{-1}(y_{2}, m(y_{1})) Example 2: Real NVP (Real-valued Non Volume Preserving) \u00b6 Affine coupling operations are (there is one translation component and one scale component for y_{2} y_{2} ): y_{1} = x_{1} y_{1} = x_{1} y_{2} = x_{2} \\odot exp(s(x_{1})) + t(x_{1}) y_{2} = x_{2} \\odot exp(s(x_{1})) + t(x_{1}) The Jacobian becomes \\frac{\\partial y}{\\partial x} = \\begin{bmatrix} I_{d} & 0 \\\\ \\frac{\\partial y_{2}}{\\partial x_{1}} & diag(exp[s(x_{1})]) \\end{bmatrix} \\frac{\\partial y}{\\partial x} = \\begin{bmatrix} I_{d} & 0 \\\\ \\frac{\\partial y_{2}}{\\partial x_{1}} & diag(exp[s(x_{1})]) \\end{bmatrix} Since the Jacobian matrix is not always equal to 1, affine coupling is not always volume preserving which is more realistic in real world data. The inverse operations are: x_{1} = y_{1} x_{1} = y_{1} x_{2} = (y_{2} - t(y_{1})) \\odot exp(-s(y_{1})) x_{2} = (y_{2} - t(y_{1})) \\odot exp(-s(y_{1})) References \u00b6 https://www.youtube.com/watch?v=RPkf516rXgw https://www.youtube.com/watch?v=PCfHd0Ec6M4 https://stackoverflow.com/questions/54635355/what-does-log-prob-do https://www.youtube.com/watch?v=uXY18nzdSsM&t=169s","title":"**Normalizing Flows and Its Friends (Part 2)**"},{"location":"MyBlogs/nf2/#normalizing-flows-and-its-friends-part-2","text":"","title":"Normalizing Flows and Its Friends (Part 2)"},{"location":"MyBlogs/nf2/#what-is-normalizing-flows","text":"One type of generative models is flow-based models, which explicitly learns the mapping between a group of samples x_{1} x_{1} , x_{2} x_{2} , ..., x_{n} x_{n} to their latent distribution p(x) p(x) . Normalizing flows is one example of flow-based models.","title":"What is normalizing flows?"},{"location":"MyBlogs/nf2/#why-normalizing-flows","text":"Given some real world data samples, we are always interested in learning its underlying distributions. Knowing the exact data distribution p(x) p(x) is helpful in some scenarios such as sampling data and identifying bias. Objective To minimize some notion of distance between p_{D} p_{D} and p_{M} p_{M} Given a dataset X=x_{1}, x_{2}, x_{3}, ... X=x_{1}, x_{2}, x_{3}, ... from an underlying distribution p_{D} p_{D} , Can we find an approximating distribution p_{M} p_{M} , which is from a family of M M and parametrized by \\theta \\theta , to minimize the distance between p_{D} p_{D} and p_{M} p_{M} ? Mathmatically, it is written as below \\theta^{*}=\\underset{\\theta \\in M}{\\mathrm{argmin}} \\space dist(p_{\\theta}, p_{D}$) \\theta^{*}=\\underset{\\theta \\in M}{\\mathrm{argmin}} \\space dist(p_{\\theta}, p_{D}$) If KL-divergence is the distance function, the above equation becomes the maximum likelihood estimation ... \\theta^{*}=\\underset{\\theta \\in M}{\\mathrm{argmin}} \\space E_{x~p_{D}} [-log \\space p_{\\theta}(x)] \\theta^{*}=\\underset{\\theta \\in M}{\\mathrm{argmin}} \\space E_{x~p_{D}} [-log \\space p_{\\theta}(x)] Flow-based models are different than GAN or VAE, since Flow-based models explicitly learn p(x) p(x) by optimizing the log likelihood. In GAN, the probability density function estimation is implicit by having the minmax classification error. We don't explicitly assign a probability density function and estimate it. In VAE, we get an approximate probability density function by optimizing the evidence lower bound (which is p_{\\theta}(x|z) p_{\\theta}(x|z) ). The encoder captures the approximate posterior mapping between x x and z z , which is q q parameterized by \\phi \\phi . The decoder captures p_{\\theta}(x|z) p_{\\theta}(x|z) which is parameterized by \\theta \\theta . In this casce, it is an approximate density estimation. In short, both GAN and VAE do not explicitly learn probability density function of real data p(x) p(x) . In flow-based models, given an x x , we want to find the function f f to get the latent representation z z . And if we invert f f , we will get x x back. The function f(x) f(x) and f^{-1}_(x) f^{-1}_(x) are exactly the inverse. And the flow-based models try to capture f f","title":"Why normalizing flows?"},{"location":"MyBlogs/nf2/#how-does-it-work","text":"We try to identify a transformation f: Z \\rightarrow X f: Z \\rightarrow X where f is a series of differentiable and invertible functions ( f_{1} f_{1} , f_{2} f_{2} , ..., f_{K} f_{K} ,) In general, for any invertible function f: Z \\rightarrow X f: Z \\rightarrow X , the probability function is below. The detailed steps can be found in this page p(x') = \\pi(z') \\mid {det(J_{f^{-1}})} \\mid p(x') = \\pi(z') \\mid {det(J_{f^{-1}})} \\mid log \\space p(x') = log \\space \\pi(z') + \\sum^{K}_{i=1} log \\mid {det(J_{f^{-1}})} \\mid log \\space p(x') = log \\space \\pi(z') + \\sum^{K}_{i=1} log \\mid {det(J_{f^{-1}})} \\mid Intuition: The first term describes the transformation f f moulds the density p_{Z}(z) p_{Z}(z) into p_{X}(x) p_{X}(x) . The second term describes the relative change of volume around z z In summary, the three requirements must hold for a normalizing flow model: Transformation function f f should be differentiable Transformation function f f should be invertible Determinant of Jacobian should be easy to compute","title":"How does it work?"},{"location":"MyBlogs/nf2/#example-1-nice-non-linear-independent-components-estimation","text":"Coupling layer operation: y_{1} = x_{1} y_{2} = g(x_{2};m(x_{1})) y_{2} = g(x_{2};m(x_{1})) Therefore, its Jacobian is a lower-triangular matrix and the determinant is the product of diagonal elements \\frac{\\partial y}{\\partial x} = \\begin{bmatrix} I & 0 \\\\ \\frac{\\partial y_{2}}{\\partial x_{1}} & \\frac{\\partial y_{2}}{\\partial x_{2}} \\end{bmatrix} \\frac{\\partial y}{\\partial x} = \\begin{bmatrix} I & 0 \\\\ \\frac{\\partial y_{2}}{\\partial x_{1}} & \\frac{\\partial y_{2}}{\\partial x_{2}} \\end{bmatrix} The inverse mappings are: x_{1} = y_{1} x_{1} = y_{1} x_{2} = g^{-1}(y_{2}, m(y_{1})) x_{2} = g^{-1}(y_{2}, m(y_{1}))","title":"Example 1: NICE (Non-linear Independent Components Estimation)"},{"location":"MyBlogs/nf2/#example-2-real-nvp-real-valued-non-volume-preserving","text":"Affine coupling operations are (there is one translation component and one scale component for y_{2} y_{2} ): y_{1} = x_{1} y_{1} = x_{1} y_{2} = x_{2} \\odot exp(s(x_{1})) + t(x_{1}) y_{2} = x_{2} \\odot exp(s(x_{1})) + t(x_{1}) The Jacobian becomes \\frac{\\partial y}{\\partial x} = \\begin{bmatrix} I_{d} & 0 \\\\ \\frac{\\partial y_{2}}{\\partial x_{1}} & diag(exp[s(x_{1})]) \\end{bmatrix} \\frac{\\partial y}{\\partial x} = \\begin{bmatrix} I_{d} & 0 \\\\ \\frac{\\partial y_{2}}{\\partial x_{1}} & diag(exp[s(x_{1})]) \\end{bmatrix} Since the Jacobian matrix is not always equal to 1, affine coupling is not always volume preserving which is more realistic in real world data. The inverse operations are: x_{1} = y_{1} x_{1} = y_{1} x_{2} = (y_{2} - t(y_{1})) \\odot exp(-s(y_{1})) x_{2} = (y_{2} - t(y_{1})) \\odot exp(-s(y_{1}))","title":"Example 2: Real NVP (Real-valued Non Volume Preserving)"},{"location":"MyBlogs/nf2/#references","text":"https://www.youtube.com/watch?v=RPkf516rXgw https://www.youtube.com/watch?v=PCfHd0Ec6M4 https://stackoverflow.com/questions/54635355/what-does-log-prob-do https://www.youtube.com/watch?v=uXY18nzdSsM&t=169s","title":"References"},{"location":"MyBlogs/rle/","text":"Paper Reading: Human Pose Regression with Residual Log-likelihood Estimation (ICCV 2021 Oral) \u00b6 Summary \u00b6 Regression problem can be reviewed from the maximum likelihood estimation perspective. The learning process of a model is to optimize its learnable parameters \\phi \\phi that makes labels \\mu_{g} \\mu_{g} most probable. If we assume the ground truth follows a laplacian distrbution, then the loss funciton is L1 L1 . If we assume the ground truth follows a gaussian distribution, then the loss function is L2 L2 . From this perspective, the loss function depends on the shape of distribution P_{\\phi}(x | I) P_{\\phi}(x | I) However, in reality, the number of samples are infinite and the real underlying distribution are unknown. It is hard to compute the real distribution of the data. In this paper, the authors take all deviations (i.e. the differences between predictions and ground truth) as samples, feed them to a flow-based model, so that the model is able to learn a mapping between simple distribution and a real distribution of the deviations. Three different designs are proposed to achieve this. Finally, a new loss function is formed, which is parameterized by the distribution of deviations. Heatmap-based or Regression-based Human Pose Estimation? \u00b6 In heatmap-based approach, the ground truth, which is usually represented by K K number of discrete 2D points, is firstly converted into K K number of continous guassian heatmap. The heatmaps have dimension of [K, H, W] [K, H, W] where H H and W W stand for height and width, respectively. During model training, ML models predict K K number of feature maps, and element-wisely compare the results with the gaussian heatmaps. In this type of approach, the dimension of the gaussian heatmap directly affects the prediction accuracy, and it is also expensive to compute and save the heatmaps.Hence, the heatmap is a bottleneck. In regression-based approach, all of the human joints ( (K joints, 3) ) are directly predicted by a model. With L1 L1 or L2 L2 losses, the loss value are directly compuated by comparing predicted coordinates and the ground truth. Since there are no needs for heatmaps in this regression-based approach, it is more widely used in practicse. In general, heatmap-based approach is able to deliver a better prediction than the regression-based approach. This is largely because a heatmap-based model only needs to predict a group of continous feature map, who should be as close as the ground truth gaussian maps. In other words, the objective of the model is to learn how to convert a given 2D image into a group of specific gaussian maps by filtering this image. The convolutional layers are really good at doing this type of filtering procedure. Mathmatically, it also means that the distribution of the ground truth are solidly defined (which is a guassian distribution). We just need to design the ML models and loss functions in a way such that the final predictions get to this distirbution as closely as possible. In a quick summary, the table below lists the advantages and disadvantages of both approaches Method Name Heatmap-based Approach Regression-based Approach Output space Discrete Continuous Computational Overhead High Low Prediction Accuracy High Low Expandability Low High Normalizing flows \u00b6 An introduction on normalization flows can be found here. 1.Basic Design \u00b6 Basically, a flow model is able to convert a simple distribution into a complex distribution. The theory behind it proves that any distributions can be modeled as long as the flow model is sufficiently complex. Because of this theory, the regression model can firstly predict a simple distribtuion. An example would be the gaussian distribution, which is defined by \\hat{\\mu} \\hat{\\mu} and \\hat{\\sigma} \\hat{\\sigma} . Next, the normalizing flows model transforms this simple distribution to the real distribution through a smooth and invertible mapping f_{\\phi} f_{\\phi} In other words, the probability density function P_{\\theta, \\phi} (x | I) P_{\\theta, \\phi} (x | I) depends on both the regression model \\theta \\theta and the flow model f_{\\phi} f_{\\phi} . Mathmatically, it is written as: Hence, the maximum likelihood estimation is: 2.Reparameterization \u00b6 The design a is not feasible in practise, because the f_{\\phi} f_{\\phi} will learn to fit the distribution of \\mu_{g} \\mu_{g} across all images. However, the distribution that we are interested in is how the output deviates from the ground truth conditioning on the input image , not distribution of the ground truth across all input images. Assuming all the underlying distributions share the same density function family but with different mean and variance conditioning on the input I I . The flow model firstly maps a zero-mean initial distribution N(0, I) N(0, I) to a zero-mean deformed distribution P_{\\phi}(x) P_{\\phi}(x) . Then, the regression model predicts a translation parameter \\hat{\\mu} \\hat{\\mu} and scale parameter \\hat{\\sigma} \\hat{\\sigma} to control the position of the deformed distribution. In other words, \\hat{\\mu} \\hat{\\mu} and \\hat{\\sigma} \\hat{\\sigma} , together, describe the the amount of deviation between predictions and ground truth. The final distribution P_{\\theta, \\phi} (x | I) P_{\\theta, \\phi} (x | I) is obtained from x = \\bar{x} * \\hat{\\sigma} + \\hat{\\mu} x = \\bar{x} * \\hat{\\sigma} + \\hat{\\mu} Hence, the maximum likelihood estimation is: where \\bar{\\mu}_{g} = (\\mu_{g} - \\hat{\\mu}) / \\hat{\\sigma} \\bar{\\mu}_{g} = (\\mu_{g} - \\hat{\\mu}) / \\hat{\\sigma} \\partial \\bar{\\mu}_{g} / \\partial \\mu_{g} = 1 / \\hat{\\sigma} \\mu_{g} \\mu_{g} is ground truth from annotation \\hat{\\mu} \\hat{\\mu} and \\hat{\\sigma} \\hat{\\sigma} , are the amount of deviation between predictions and ground truth \\bar{\\mu}_{g} \\bar{\\mu}_{g} is the ground truth used in the objective function. It is a result, computed by subtracting each ground truth from deviation. The flow model takes these samples, and tries to learn this distribution. 3.Residual log-likelihood estimation \u00b6 The training of \\hat{\\mu} \\hat{\\mu} and \\hat{\\sigma} \\hat{\\sigma} is closely tied with the training of flow model. At the beginning of the training, the distribution shape is far from correct, which increases the difficulty to train the regresson model. A gradient shortcut was developed to reduce the dependencies between the flow model and regression model. P_{\\phi}(\\bar{x}) P_{\\phi}(\\bar{x}) is trying to fit the optimal underlying distribution P_{opt}(\\bar{x}) P_{opt}(\\bar{x}) where Q(\\bar{x}) Q(\\bar{x}) can be a simple distribution, i.e. Gaussian or Laplacian distribution log \\frac{P_{opt}(\\bar{x})}{s*Q(\\bar{x})} log \\frac{P_{opt}(\\bar{x})}{s*Q(\\bar{x})} is called residual log-likelihood log s log s , is constant, which makes the whole equation stay as a distribution The assumption is that the simple distribution Q(\\bar{x}) Q(\\bar{x}) is a rough estimation of the underlying distribution. And the residual log-likelihood is to finetune or compensite for the difference. where G_{\\phi}(\\bar{x}) G_{\\phi}(\\bar{x}) is the distribution learned by the flow model. In this way, G_{\\phi}(\\bar{x}) G_{\\phi}(\\bar{x}) will try to fintune the overal distribution Q(\\bar{x}) Q(\\bar{x}) , instead of directly fitting the underlying distribution. Hence, the maximum likelihood estimation is: By doing this, The backward propagation of Q(\\bar{x}) Q(\\bar{x}) does not depend on the flow model. It is also easier to train the residual mapping than train the original unreferenced mapping. Implementations \u00b6 The initial density is set to be a zero-mean N(0, I) N(0, I) During inference, the regression model predict \\hat{\\mu} \\hat{\\mu} , which can directly be used as regressed outputs. No need for the flow model. Some Technical Details \u00b6 The value of Residual log-likelihood can be positive or negative number, because of L=-log p L=-log p . The term becomes negative when p p is larger than 1 In bar_mu = (src_poses - gt_uvd) / sigma , coordinate normalization is performed. A sigmoid operation is also performed on sigma . REF: https://zhuanlan.zhihu.com/p/395521994 https://zhuanlan.zhihu.com/p/429017412","title":"Paper Reading: Human Pose Regression with Residual Log-likelihood Estimation (ICCV 2021 Oral)"},{"location":"MyBlogs/rle/#paper-reading-human-pose-regression-with-residual-log-likelihood-estimation-iccv-2021-oral","text":"","title":"Paper Reading: Human Pose Regression with Residual Log-likelihood Estimation (ICCV 2021 Oral)"},{"location":"MyBlogs/rle/#summary","text":"Regression problem can be reviewed from the maximum likelihood estimation perspective. The learning process of a model is to optimize its learnable parameters \\phi \\phi that makes labels \\mu_{g} \\mu_{g} most probable. If we assume the ground truth follows a laplacian distrbution, then the loss funciton is L1 L1 . If we assume the ground truth follows a gaussian distribution, then the loss function is L2 L2 . From this perspective, the loss function depends on the shape of distribution P_{\\phi}(x | I) P_{\\phi}(x | I) However, in reality, the number of samples are infinite and the real underlying distribution are unknown. It is hard to compute the real distribution of the data. In this paper, the authors take all deviations (i.e. the differences between predictions and ground truth) as samples, feed them to a flow-based model, so that the model is able to learn a mapping between simple distribution and a real distribution of the deviations. Three different designs are proposed to achieve this. Finally, a new loss function is formed, which is parameterized by the distribution of deviations.","title":"Summary"},{"location":"MyBlogs/rle/#heatmap-based-or-regression-based-human-pose-estimation","text":"In heatmap-based approach, the ground truth, which is usually represented by K K number of discrete 2D points, is firstly converted into K K number of continous guassian heatmap. The heatmaps have dimension of [K, H, W] [K, H, W] where H H and W W stand for height and width, respectively. During model training, ML models predict K K number of feature maps, and element-wisely compare the results with the gaussian heatmaps. In this type of approach, the dimension of the gaussian heatmap directly affects the prediction accuracy, and it is also expensive to compute and save the heatmaps.Hence, the heatmap is a bottleneck. In regression-based approach, all of the human joints ( (K joints, 3) ) are directly predicted by a model. With L1 L1 or L2 L2 losses, the loss value are directly compuated by comparing predicted coordinates and the ground truth. Since there are no needs for heatmaps in this regression-based approach, it is more widely used in practicse. In general, heatmap-based approach is able to deliver a better prediction than the regression-based approach. This is largely because a heatmap-based model only needs to predict a group of continous feature map, who should be as close as the ground truth gaussian maps. In other words, the objective of the model is to learn how to convert a given 2D image into a group of specific gaussian maps by filtering this image. The convolutional layers are really good at doing this type of filtering procedure. Mathmatically, it also means that the distribution of the ground truth are solidly defined (which is a guassian distribution). We just need to design the ML models and loss functions in a way such that the final predictions get to this distirbution as closely as possible. In a quick summary, the table below lists the advantages and disadvantages of both approaches Method Name Heatmap-based Approach Regression-based Approach Output space Discrete Continuous Computational Overhead High Low Prediction Accuracy High Low Expandability Low High","title":"Heatmap-based or Regression-based Human Pose Estimation?"},{"location":"MyBlogs/rle/#normalizing-flows","text":"An introduction on normalization flows can be found here.","title":"Normalizing flows"},{"location":"MyBlogs/rle/#1basic-design","text":"Basically, a flow model is able to convert a simple distribution into a complex distribution. The theory behind it proves that any distributions can be modeled as long as the flow model is sufficiently complex. Because of this theory, the regression model can firstly predict a simple distribtuion. An example would be the gaussian distribution, which is defined by \\hat{\\mu} \\hat{\\mu} and \\hat{\\sigma} \\hat{\\sigma} . Next, the normalizing flows model transforms this simple distribution to the real distribution through a smooth and invertible mapping f_{\\phi} f_{\\phi} In other words, the probability density function P_{\\theta, \\phi} (x | I) P_{\\theta, \\phi} (x | I) depends on both the regression model \\theta \\theta and the flow model f_{\\phi} f_{\\phi} . Mathmatically, it is written as: Hence, the maximum likelihood estimation is:","title":"1.Basic Design"},{"location":"MyBlogs/rle/#2reparameterization","text":"The design a is not feasible in practise, because the f_{\\phi} f_{\\phi} will learn to fit the distribution of \\mu_{g} \\mu_{g} across all images. However, the distribution that we are interested in is how the output deviates from the ground truth conditioning on the input image , not distribution of the ground truth across all input images. Assuming all the underlying distributions share the same density function family but with different mean and variance conditioning on the input I I . The flow model firstly maps a zero-mean initial distribution N(0, I) N(0, I) to a zero-mean deformed distribution P_{\\phi}(x) P_{\\phi}(x) . Then, the regression model predicts a translation parameter \\hat{\\mu} \\hat{\\mu} and scale parameter \\hat{\\sigma} \\hat{\\sigma} to control the position of the deformed distribution. In other words, \\hat{\\mu} \\hat{\\mu} and \\hat{\\sigma} \\hat{\\sigma} , together, describe the the amount of deviation between predictions and ground truth. The final distribution P_{\\theta, \\phi} (x | I) P_{\\theta, \\phi} (x | I) is obtained from x = \\bar{x} * \\hat{\\sigma} + \\hat{\\mu} x = \\bar{x} * \\hat{\\sigma} + \\hat{\\mu} Hence, the maximum likelihood estimation is: where \\bar{\\mu}_{g} = (\\mu_{g} - \\hat{\\mu}) / \\hat{\\sigma} \\bar{\\mu}_{g} = (\\mu_{g} - \\hat{\\mu}) / \\hat{\\sigma} \\partial \\bar{\\mu}_{g} / \\partial \\mu_{g} = 1 / \\hat{\\sigma} \\mu_{g} \\mu_{g} is ground truth from annotation \\hat{\\mu} \\hat{\\mu} and \\hat{\\sigma} \\hat{\\sigma} , are the amount of deviation between predictions and ground truth \\bar{\\mu}_{g} \\bar{\\mu}_{g} is the ground truth used in the objective function. It is a result, computed by subtracting each ground truth from deviation. The flow model takes these samples, and tries to learn this distribution.","title":"2.Reparameterization"},{"location":"MyBlogs/rle/#3residual-log-likelihood-estimation","text":"The training of \\hat{\\mu} \\hat{\\mu} and \\hat{\\sigma} \\hat{\\sigma} is closely tied with the training of flow model. At the beginning of the training, the distribution shape is far from correct, which increases the difficulty to train the regresson model. A gradient shortcut was developed to reduce the dependencies between the flow model and regression model. P_{\\phi}(\\bar{x}) P_{\\phi}(\\bar{x}) is trying to fit the optimal underlying distribution P_{opt}(\\bar{x}) P_{opt}(\\bar{x}) where Q(\\bar{x}) Q(\\bar{x}) can be a simple distribution, i.e. Gaussian or Laplacian distribution log \\frac{P_{opt}(\\bar{x})}{s*Q(\\bar{x})} log \\frac{P_{opt}(\\bar{x})}{s*Q(\\bar{x})} is called residual log-likelihood log s log s , is constant, which makes the whole equation stay as a distribution The assumption is that the simple distribution Q(\\bar{x}) Q(\\bar{x}) is a rough estimation of the underlying distribution. And the residual log-likelihood is to finetune or compensite for the difference. where G_{\\phi}(\\bar{x}) G_{\\phi}(\\bar{x}) is the distribution learned by the flow model. In this way, G_{\\phi}(\\bar{x}) G_{\\phi}(\\bar{x}) will try to fintune the overal distribution Q(\\bar{x}) Q(\\bar{x}) , instead of directly fitting the underlying distribution. Hence, the maximum likelihood estimation is: By doing this, The backward propagation of Q(\\bar{x}) Q(\\bar{x}) does not depend on the flow model. It is also easier to train the residual mapping than train the original unreferenced mapping.","title":"3.Residual log-likelihood estimation"},{"location":"MyBlogs/rle/#implementations","text":"The initial density is set to be a zero-mean N(0, I) N(0, I) During inference, the regression model predict \\hat{\\mu} \\hat{\\mu} , which can directly be used as regressed outputs. No need for the flow model.","title":"Implementations"},{"location":"MyBlogs/rle/#some-technical-details","text":"The value of Residual log-likelihood can be positive or negative number, because of L=-log p L=-log p . The term becomes negative when p p is larger than 1 In bar_mu = (src_poses - gt_uvd) / sigma , coordinate normalization is performed. A sigmoid operation is also performed on sigma . REF: https://zhuanlan.zhihu.com/p/395521994 https://zhuanlan.zhihu.com/p/429017412","title":"Some Technical Details"}]}